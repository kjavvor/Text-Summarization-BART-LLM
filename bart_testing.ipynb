{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing BART Model Performance After Fine-Tuning\n",
    "\n",
    "In this notebook, we evaluate the BART model's ability to generate summaries after undergoing fine-tuning on custom user data. The analysis is based on popular text quality metrics, specifically **ROUGE** and **BLEU**.\n",
    "\n",
    "### Experiment Objective:\n",
    "\n",
    "1. Compare the quality of summaries generated by:\n",
    "   - The pre-trained BART model.\n",
    "   - The BART model fine-tuned on the user's dataset.\n",
    "2. Use the same test examples for evaluating both models.\n",
    "3. Calculate and compare ROUGE and BLEU scores to better understand the performance differences.\n",
    "\n",
    "The notebook begins by preparing the necessary tools and downloading resources required for computing the evaluation metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## Preparing the Environment for Metric Computation\n",
    "\n",
    "The following code installs and sets up the NLTK library to enable sentence and word tokenization. This is a crucial step, as ROUGE and BLEU metrics require proper text preprocessing to accurately compare reference and generated summaries.\n",
    "\n",
    "### Detailed explanation:\n",
    "\n",
    "1. **Importing NLTK**:\n",
    "   - `nltk` (Natural Language Toolkit) is a widely used library for natural language processing, including tasks such as text tokenization.\n",
    "\n",
    "2. **Downloading NLTK resources**:\n",
    "   - `nltk.download('punkt')` downloads the `punkt` resource, which enables sentence and word tokenization.\n",
    "   - This is required for subsequent BLEU and ROUGE metric calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kamiljaworski/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Summaries Generated by the BART Model\n",
    "\n",
    "This code loads a previously saved BART model, generates a summary for a randomly selected article from the test set, and compares it with both the original article and the reference summary from the dataset.\n",
    "\n",
    "### Detailed explanation:\n",
    "\n",
    "1. **Function `load_model`**:\n",
    "   - Loads a fine-tuned BART model and its tokenizer from the specified directory.\n",
    "   - **Arguments**:\n",
    "     - `model_name`: Name of the directory containing the saved model.\n",
    "     - `base_dir`: Base path where models are stored (default: `./models`).\n",
    "   - **Returns**:\n",
    "     - `model`: The loaded BART model.\n",
    "     - `tokenizer`: The loaded tokenizer.\n",
    "\n",
    "2. **Function `load_test_data`**:\n",
    "   - Loads test data from a JSON file.\n",
    "   - **Arguments**:\n",
    "     - `file_path`: Path to the test JSON file.\n",
    "   - **Returns**:\n",
    "     - A list of articles from the test dataset.\n",
    "\n",
    "3. **Function `generate_summary`**:\n",
    "   - Uses the BART model to generate a summary for a given text.\n",
    "   - **Arguments**:\n",
    "     - `model`: The fine-tuned BART model.\n",
    "     - `tokenizer`: The tokenizer associated with the model.\n",
    "     - `text`: The input article text to summarize.\n",
    "     - `max_length`: Maximum length of the generated summary (default: 150 tokens).\n",
    "     - `min_length`: Minimum length of the generated summary (default: 40 tokens).\n",
    "   - **Returns**:\n",
    "     - A string containing the generated summary.\n",
    "\n",
    "4. **Environment setup**:\n",
    "   - The model and tokenizer are loaded from the `model_v3` directory.\n",
    "   - The model is moved to the appropriate device (`mps` on macOS or `cpu` otherwise).\n",
    "   - Test data is loaded from the file located in `./datasets/splits_filtered_with_summary/test.json`.\n",
    "\n",
    "5. **Generating and comparing summaries**:\n",
    "   - A random article is selected from the test set.\n",
    "   - For that article:\n",
    "     - The original article text is retrieved (`original_text`).\n",
    "     - The reference summary from the dataset is retrieved (`dataset_summary`).\n",
    "     - A new summary is generated by the model (`generated_summary`).\n",
    "\n",
    "6. **Displaying results**:\n",
    "   - The comparison is printed in the following format:\n",
    "     - **Original Text** – the full article.\n",
    "     - **Dataset Summary** – the reference summary from the dataset.\n",
    "     - **Generated Summary by Model** – the summary produced by the fine-tuned model.\n",
    "   - Texts are formatted using `textwrap.fill` to limit line width to 80 characters for better readability.\n",
    "\n",
    "### Result:\n",
    "- This script enables direct comparison between the model-generated summary and the human-written reference summary, allowing for a subjective evaluation of the model's performance.\n",
    "\n",
    "### Notes:\n",
    "- The model and tokenizer must be saved in the directory `./models/{model_name}` prior to running this script.\n",
    "- The test data must include `text` and `summary` fields for each article.\n",
    "- The `generate_summary` function uses beam search (`num_beams=4`) to improve the quality of generated text, at the cost of higher computational load.\n",
    "- The code is flexible and can be easily adapted for testing more articles or adjusting generation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Comparison ===\n",
      "\n",
      "Original Text:\n",
      "And we turn now to Lebanon. Lebanon takes stock of the damage done by the war\n",
      "between Israel and Hezbollah. It finds it's always complicated political\n",
      "situation has shaken up even more than usual. After a year of pushing for\n",
      "democratic reforms, Prime Minister Fouad Siniora is on the defensive as\n",
      "opposition leaders call for his resignation. Hezbollah, meanwhile, is riding a\n",
      "wave of popularity for its battle with Israel, and the movement is showing no\n",
      "inclination to turn in its weapons. Analysts say the need to strengthen the\n",
      "Lebanese state is more urgent than ever but no easier. NPR's Peter Kenyon\n",
      "reports from Beirut. Since the war started in July, Prime Minister Siniora has\n",
      "seen his popularity rise as Lebanese rallied around their government under the\n",
      "strain of the Israeli military bombardment. Siniora is part of the so-called\n",
      "March 14th Alliance, named for the date of the massive Beirut rally that united\n",
      "the opposition in the wake of the assassination of former Prime Minister Rafiq\n",
      "Al-Hariri. But in the post-war period, the government has been swamped with the\n",
      "financial and logistical needs of the multibillion-dollar rebuilding project\n",
      "Lebanon now faces. Siniora's critics, meanwhile, are on the attack. One leading\n",
      "Christian politician, Michel Aoun, who's made an alliance with Hezbollah, is\n",
      "calling for Siniora to resign. Lebanese lawmaker Ibrahim Kenaan, an Aoun\n",
      "supporter, says only a national unity government will have the strength to stand\n",
      "up to the outside players that he says are using Lebanon for their own ends. I\n",
      "think the current government is actually from the past. Yes, there is regional\n",
      "forces that are really fighting on our land - Syria, probably Iran. Israel. And\n",
      "because of the absence of a strong Lebanese state, and because of the absence of\n",
      "a Lebanese vision and a Lebanese program, how can we deal with them? Siniora\n",
      "responded by saying the government had no intention of resigning, and he called\n",
      "on all parties to respect Lebanese sovereignty. But the prime minister has also\n",
      "spoken candidly in recent days about the weakness of the Lebanese state. On one\n",
      "level, the problems facing Siniora and the March 14th forces were evident well\n",
      "before Hezbollah provoked this war. The anger and grief in the wake of the Rafiq\n",
      "Hariri assassination that united the Druze, Sunni and some Christian parties in\n",
      "the struggle to oust the Syrian military from Lebanon evaporated once the\n",
      "alliance achieved a majority in the government. Old in-fighting resurfaced and\n",
      "political progress had all but stalled when the war broke out. Now some of the\n",
      "governing alliance fear that with Lebanon in post-war disarray, Syria could try\n",
      "to regain its influence politically if not militarily. Wael Abu-Faour, a deputy\n",
      "with the March 14th Alliance, believes Damascus will try anything, including\n",
      "toppling the current Lebanese government, to avoid facing an international\n",
      "tribunal in the Rafiq Hariri assassination, currently being investigated by the\n",
      "United Nations. Prosecutor Serge Brammertz is due to report on his findings in\n",
      "the case next month. Faour says if the Syrians begin to worry that his report\n",
      "will reach high up into the regime of President Bashar al-Assad, Lebanon may see\n",
      "a return to the explosions and assassinations that were Beirut's nightmare\n",
      "before this latest war. They will try to escape this court and to change the\n",
      "majority in Lebanon to make this government doubtful. And I think that the\n",
      "Syrian regime, if they will not succeed in changing the government, they will go\n",
      "back to their scenario of killing. Syria denies any involvement in the Hariri\n",
      "assassination or the killing of anti-Syrian politicians and journalists in\n",
      "Beirut that followed. Analyst Paul Salem, director-designate of the new Carnegie\n",
      "Middle East Center in Beirut, says the war helped many Lebanese realize that the\n",
      "best thing would be to have a strong state that can speak for Lebanon's\n",
      "Christian, Sunni, Shiite and Druze communities. But he wonders if Lebanon's\n",
      "eternal fractious political parties are prepared to take advantage of the\n",
      "moment. I think one lesson this war taught the Lebanese is that the Sunnis\n",
      "cannot govern alone. Like the Christians learned, you know, 20 years ago that\n",
      "they could not govern alone, that the Lebanese cannot govern without the Shiites\n",
      "being full partners. War and the after effects of the war make the issue more\n",
      "urgent. Now will the political leadership rise to the challenge? That is a big\n",
      "question. Today, as the Lebanese army takes up positions on the border with\n",
      "Israel for the first time in decades, Lebanese are wondering if it's the\n",
      "beginning of a solution or just a new version of a very old problem. Peter\n",
      "Kenyon, NPR News, Beirut. You're listening to MORNING EDITION from NPR News.\n",
      "\n",
      "Dataset Summary:\n",
      "In the wake of Israel's war with Hezbollah, Lebanon's political system is in\n",
      "turmoil. Hezbollah has gained popularity for its confrontation with Israel. And\n",
      "Prime Minister Fouad Siniora — who's long been an advocate of democratic reform\n",
      "— is on the defensive as opposition leaders call for his resignation.\n",
      "\n",
      "Generated Summary by Model:\n",
      "After a year of pushing for democratic reforms, Lebanese Prime Minister Fouad\n",
      "Siniora is on the defensive as opposition leaders call for his resignation.\n",
      "Hezbollah is riding a wave of popularity for its battle with Israel, and the\n",
      "movement is showing no inclination to turn in its weapons. Analysts say the need\n",
      "to strengthen the Lebanese state is more urgent than ever but no easier.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import textwrap\n",
    "\n",
    "# Function to load the model and tokenizer\n",
    "def load_model(model_name, base_dir=\"./models\"):\n",
    "    \"\"\"\n",
    "    Load a trained model and its tokenizer from a specified directory.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the directory containing the model.\n",
    "        base_dir (str): Base directory where models are stored.\n",
    "\n",
    "    Returns:\n",
    "        model: The BART model.\n",
    "        tokenizer: The BART tokenizer.\n",
    "    \"\"\"\n",
    "    model_path = os.path.join(base_dir, model_name)\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Function to load test data from a JSON file\n",
    "def load_test_data(file_path):\n",
    "    \"\"\"\n",
    "    Load test dataset from a JSON file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of articles from the dataset.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Function to generate a summary for a given text\n",
    "def generate_summary(model, tokenizer, text, max_length=150, min_length=40):\n",
    "    \"\"\"\n",
    "    Generate a summary for the input text using the BART model.\n",
    "\n",
    "    Args:\n",
    "        model: Trained BART model.\n",
    "        tokenizer: Tokenizer associated with the model.\n",
    "        text (str): Text to summarize.\n",
    "        max_length (int): Maximum length of the generated summary.\n",
    "        min_length (int): Minimum length of the generated summary.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated summary.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "        length_penalty=2.0,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"model_v3\"  # Replace with your model name\n",
    "model, tokenizer = load_model(model_name)\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"  # Adjust for your hardware\n",
    "model.to(device)\n",
    "\n",
    "# Load test dataset\n",
    "test_file_path = \"./datasets/splits_filtered_with_summary/test.json\"\n",
    "test_data = load_test_data(test_file_path)\n",
    "\n",
    "# Select a random article from the test dataset\n",
    "random_article = random.choice(test_data)\n",
    "\n",
    "# Retrieve the original text, dataset summary, and generate a new summary\n",
    "original_text = random_article[\"text\"]\n",
    "dataset_summary = random_article[\"summary\"]\n",
    "generated_summary = generate_summary(model, tokenizer, original_text)\n",
    "\n",
    "# Display side-by-side comparison\n",
    "print(\"=== Comparison ===\\n\")\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(textwrap.fill(original_text, width=80))\n",
    "\n",
    "print(\"\\nDataset Summary:\")\n",
    "print(textwrap.fill(dataset_summary, width=80))\n",
    "\n",
    "print(\"\\nGenerated Summary by Model:\")\n",
    "print(textwrap.fill(generated_summary, width=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BART Model Evaluation: Pre-trained vs Fine-tuned — **ROUGE**\n",
    "\n",
    "This section of the code compares the performance of the pre-trained BART model and a fine-tuned version trained on custom user data. The comparison is performed using **ROUGE** metrics, with results displayed in tabular format as well as on sample-level summaries.\n",
    "\n",
    "---\n",
    "\n",
    "### Detailed explanation:\n",
    "\n",
    "1. **Function `load_model`**:\n",
    "   - Loads the BART model and tokenizer.\n",
    "   - If `model_name` is specified, it loads a fine-tuned model from `./models/{model_name}`.\n",
    "   - Otherwise, it loads the pre-trained model (`facebook/bart-base`).\n",
    "\n",
    "2. **Function `load_test_data`**:\n",
    "   - Loads the test dataset from a JSON file.\n",
    "\n",
    "3. **Function `generate_summary`**:\n",
    "   - Generates a summary for the given input text using the BART model.\n",
    "   - Generation settings:\n",
    "     - `max_length=150`: Maximum length of the generated summary.\n",
    "     - `min_length=40`: Minimum length of the generated summary.\n",
    "     - `num_beams=2`: Number of beams used in beam search (lowered for speed during testing).\n",
    "\n",
    "4. **Function `evaluate_rouge`**:\n",
    "   - Compares model-generated summaries against reference summaries using ROUGE metrics.\n",
    "   - **Returns**:\n",
    "     - ROUGE scores computed over the dataset.\n",
    "     - A DataFrame containing individual results for each article: original text, reference summary, and generated summary.\n",
    "\n",
    "5. **Evaluation process**:\n",
    "   - Both the pre-trained and fine-tuned models are evaluated on the same subset of test data (first 10 examples).\n",
    "   - ROUGE results for each model are collected and displayed in a comparison table.\n",
    "\n",
    "6. **Comparison of results**:\n",
    "   - ROUGE scores for both models are printed side-by-side, including:\n",
    "     - **ROUGE-1**: Unigram overlap.\n",
    "     - **ROUGE-2**: Bigram overlap.\n",
    "     - **ROUGE-L** and **ROUGE-Lsum**: Longest common subsequence coverage.\n",
    "   - Evaluation time is measured for each model to provide performance insights.\n",
    "\n",
    "7. **Presentation of example outputs**:\n",
    "   - For each article in the evaluated subset, the following are printed:\n",
    "     - The original article text.\n",
    "     - The reference summary from the dataset.\n",
    "     - The summary generated by the pre-trained model.\n",
    "     - The summary generated by the fine-tuned model.\n",
    "\n",
    "---\n",
    "\n",
    "### Output:\n",
    "\n",
    "- A table comparing ROUGE scores between the pre-trained and fine-tuned BART models.\n",
    "- Detailed textual comparisons for individual examples to qualitatively assess summarization performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Notes:\n",
    "\n",
    "- **Model effectiveness**:\n",
    "  - The fine-tuned model is expected to achieve better ROUGE scores, as it has been adapted to the domain-specific data.\n",
    "- **Evaluation time**:\n",
    "  - The fine-tuned model may take longer to generate summaries depending on its complexity and tuning.\n",
    "- **Requirements**:\n",
    "  - The `evaluate` library is required to compute ROUGE metrics (`pip install evaluate`).\n",
    "  - The test dataset must contain `text` and `summary` fields for each article.\n",
    "\n",
    "This evaluation setup offers both quantitative and qualitative insights into how fine-tuning impacts the performance of the BART model on summarization tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Pre-trained BART...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 10/10 [00:18<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Fine-tuned BART...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 10/10 [00:06<00:00,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ROUGE Scores Comparison ===\n",
      "    Metric  Pre-trained BART  Fine-tuned BART\n",
      "   ROUGE-1            0.3202           0.4465\n",
      "   ROUGE-2            0.1250           0.2370\n",
      "   ROUGE-L            0.2130           0.3610\n",
      "ROUGE-Lsum            0.2138           0.3613\n",
      "\n",
      "Pre-trained BART Evaluation Time: 21.51 seconds\n",
      "Fine-tuned BART Evaluation Time: 7.99 seconds\n",
      "\n",
      "=== Sample Results Comparison ===\n",
      "\n",
      "Example 1:\n",
      "\n",
      "Original Text:\n",
      "During the presidential campaign, Donald Trump suggested that he might favor creating a database for Muslims who enter the United States. At other times, he has called for extreme vetting for people from terror-prone countries. The U.S. government actually once had a system that could serve as a model for this. After 9/11, the Bush administration established a registry called NSEERS. That stands for the National Security Entry-Exit Registration System. We're going to talk now with someone who has studied NSEERS. Muzaffar Chishti directs the Migration Policy Institute's office at NYU School of Law. That's an independent non-partisan think tank that studies migration. Welcome to the program. Thank you so much for having me. Who had to register under this program? Initially, it was confined to about five countries. It was Syria, Sudan, Iraq, Iran and Libya. It ultimately expanded to about 25 countries. Almost all of them were Muslim majority countries except for North Korea. So it was confined for nonimmigrants, which is noncitizens of the United States from those countries who are coming for temporary stay. If they entered at our ports of entry, they would be registered, interviewed, photographed, fingerprinted at special counters. And then they had to, within 30 days of the arrival, register at a local immigration office and then do it every year. When you say nonimmigrant, do you mean tourists, students? What do you mean by nonimmigrants? It could be anyone who came to the U.S. on a temporary visa, you know, short-term tourist visa to long-term professor teaching at a university or someone working at a multinational corporation. Some 80,000 people were registered under this program, is that right? All of them males, age 16 and up. That's right. The total number who came in was about 83,000. A report from Penn State law school's Rights Working Group says 13,000 people were put into removal proceedings under this program. What were those proceedings for? So when people came in for their interview, they were then screened for their immigration issues and immigration history. If any of them had violated any provision of the law, they were placed in removal proceedings. Was anyone removed for terrorism offenses under this program? Well, you know, the - one of the problems of the NSEERS program is that they lacked transparency, so we don't really know. The government never released any numbers or information on that account. There was a New York Times story at that time which concluded about 11 people were found to be of interest to the government but not knowing what that meant in terms of what kind of information we had on those people and what level of terrorist activity they were involved in. So given the limited information that we have, how do you weigh the relative costs and benefits of the program? Well, I mean, there was an Office of Inspector General of the DHS which, in 2012, did an extensive report and found that this was largely ineffective, both in terms of the nature of the database, which was really not very credible, and the yields in terms of national security interest. And it cost about $10 million a year. And the inspector general's conclusion was that it would have been better to spend that money on real targets based on real information about security threats as against broadly interrogating people just on the basis of their national origin. One of the people who helped implement this program in the Bush Justice Department, Kris Kobach, is now advising the Trump transition team on immigration issues. He was actually photographed with Trump this week holding a document that called for reintroducing the program. How do you think a 2017 version of this program might look different from what we saw during the Bush years? The issue with reinstation (ph) of this program is that how does one create a Muslim registry if that's the intent of the administration? Because we have no way of knowing, even if we want to, who is a Muslim and who's not a Muslim. The best proxies we can make are people born in certain countries of the world or, as the president-elect during the campaign pointed out, that we are looking at countries which are prone to terrorism. And today, you will say this is ISIS' presence, but ISIS is now, in one form of the present, in a lot more countries of the world, including many countries in Europe. So how do you have a registry program which includes all those countries? One of the justification for that choice would be - could be really determined by the administration and, frankly, they would have a lot of leeway in deciding what the group is. Muzaffar Chishti directs the Migration Policy Institute's office at NYU School of Law. Thank you for joining us. Thank you so much for having me.\n",
      "\n",
      "Reference Summary:\n",
      "If the Trump administration decides to implement a registry for Muslims entering the United States, it has a model: the U.S. put a registration system in place after 9/11. NPR's Ari Shapiro talks to Muzaffar Chishti, director of the Migration Policy Institute at the NYU School of Law, about the impact of that system.\n",
      "\n",
      "Generated Summary (Pre-trained BART):\n",
      "During the presidential campaign, Donald Trump suggested that he might favor creating a database for Muslims who enter the United States. At other times, he has called for extreme vetting for people from terror-prone countries. The U.S. government actually once had a system that could serve as a model for this. After 9/11, the Bush administration established a registry called NSEERS. That stands for the National Security Entry-Exit Registration System. We're going to talk now with someone who has studied NSEers. Muzaffar Chishti directs the Migration Policy Institute's office at NYU School of Law. That's an independent non-partisan think tank that studies migration. Welcome to the program. Thank you so much for\n",
      "\n",
      "Generated Summary (Fine-tuned BART):\n",
      "NPR's Audie Cornish speaks with Muzaffar Chishti, director of the Migration Policy Institute's Office at NYU School of Law, about the National Security Entry-Exit Registration System.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "\n",
      "Original Text:\n",
      "This is ALL THINGS CONSIDERED from NPR News. Im Robert Siegel. And Im Michele Norris. As BP prepares yet another attempt to cap a runaway oil spill in the Gulf of Mexico, the Obama administration continues to try to show it's on top of the situation. President Obama met today with the co-chairman of his new commission set up to study the disaster. And Attorney General Eric Holder announced a criminal investigation of whats now the worst oil spill in U.S. history. As NPRs Scott Horsley reports, theres no quick relief in sight. BP has failed time and again to staunch the oil gushing from its broken well. Even if the latest effort is successful, it's likely to increase the flow of oil in the short run. That's left the Obama administration in the uncomfortable position of having to clean up a mess it's been powerless to prevent. President Obama returned today to an itemized list of what he called the largest government cleanup effort in the nation's history. We've authorized more than 17,000 National Guard members to respond across four states. More than 1,700 vessels are currently aiding in the response and will ensure that any and all responsible means of containing this leak are pursued as we await the completion of the two relief wells. Mr. Obama spoke in the Rose Garden after meeting with two men he's tapped to lead a presidential commission on the oil spill. Former Florida Governor Bob Graham and former EPA administrator William Reilly will investigate the causes of this disaster and how to prevent something like it from ever happening again. If our laws were broken, leading to this death and destruction, my solemn pledge is that we will bring those responsible to justice on behalf of the victims of this catastrophe and the people of the Gulf region. To dramatize that determination, Attorney General Eric Holder made a personal visit to the Gulf today. He announced the government has been conducting a criminal probe of the spill for some weeks We will closely examine the actions of those involved in this spill. If we find evidence of illegal behavior, we will be extremely forceful in our response. Also today, the government began hosting its own daily briefing on the spill. Retired Coast Guard Admiral Thad Allen will update reporters by himself instead of alongside BP officials, as the Coast Guard has done in the past. The goal is to create as broad a picture of this response, what's going on and speak very frankly to the American public, be able to answer your questions in my role as a national incident commander. The goal may also be to put more distance between the administration and BP. Otherwise, mounting frustration with the oil company is likely to continue to spill over on the government, says political analyst Jack Pitney of Claremont McKenna College. The president's basic problem is the gap between expectations and reality. The president obviously built up a lot of expectations about what he could do in general. And now he's facing a problem that doesn't lend itself to an easy fix. The president reportedly has fumed in private about BP's inability to shut off the well. But he struggles to communicate his frustration publicly, as he did this morning with a rather colorless description of the unique Gulf lifestyle now at risk. What's being threatened, what's being lost isn't just the source of income but a way of life, not just fishable waters, but a national treasure. Analyst Pitney says this is one instance where a little more emotion from the president might be welcome. The president is famous for his reserve, for his steady control of his emotions. And in most respects, that's a virtue. But in this case he has to show empathy, he has to show that he cares about the people of the Gulf Coast. Still, Pitney says there are limits to that approach. Even the most emotionally demonstrative president could wind up stained by the oil spill until the engineers succeed in shutting off the leak. Scott Horsley, NPR News, the White House.\n",
      "\n",
      "Reference Summary:\n",
      "President Obama met Tuesday with the co-chairs of the BP oil spill commission, and said that an independent commission will thoroughly examine all aspects of the disaster. Meanwhile, he's dispatched Attorney General Eric Holder to the Gulf Coast. Holder said Tuesday that federal authorities have opened criminal and civil investigations into the spill.\n",
      "\n",
      "Generated Summary (Pre-trained BART):\n",
      "This is ALL THINGS CONSIDERED from NPR News. Im Robert Siegel. And Im Michele Norris. As BP prepares yet another attempt to cap a runaway oil spill in the Gulf of Mexico, the Obama administration continues to try to show it's on top of the situation. President Obama met today with the co-chairman of his new commission set up to study the disaster. And Attorney General Eric Holder announced a criminal investigation of the spill for some weeks. The spill is now the worst oil spill of U.S. history. As NPRs Scott Horsley reports, theres no quick relief in sight. BP has failed time and again to staunch the oil gushing from its broken well. Even if the latest effort\n",
      "\n",
      "Generated Summary (Fine-tuned BART):\n",
      "President Obama met with the co-chairman of his new commission set up to study the disaster in the Gulf of Mexico on Wednesday. The president also met with two men he's tapped to lead the commission on the disaster.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "\n",
      "Original Text:\n",
      "Fred Thompson says he's still testing the waters for a presidential run. But until yesterday, he hadn't dipped so much as a toe in Iowa. Mr. Thompson made a stop at the state fair in Des Moines on Friday, nearly a week after he skipped the Iowa Straw Poll. Republicans who are still not satisfied with their choices appear to be receptive to what the former senator from Tennessee has to say. NPR's Audie Cornish reports. Judging from the opening anecdotes at the speech at the Iowa state fair, actor and former Senator Fred Thompson knows \"Law & Order\" reruns aren't going to be enough to keep him afloat while he builds up to a presidential campaign. I was at an airport recently and a lady came up and said, we just loved your TV show. You meant so much to me and my family, I just can't tell you what big fans we are and so forth, we're just stopping and say hello to my husband and (unintelligible) oh, certainly, certainly, I'd be glad, then the little lady said, honey, you'll never believe it, Dr. Phil. But while TV psychologist Dr. Phil McGraw gives advice, Thompson was getting it. Mr. DELL JOHNSON(ph) (Resident, Iowa): I did get a chance to talk to him. And I said quit thinking, just quit thinking about it and get it in. Dell Johnson(ph) in Minden(ph), Iowa, says he's impressed with Thompson's laid-back style but says it's time for him to commit to running. And that's a legitimate concern, says Professor Dennis Goldford of Drake University in Des Moines. Like all actors, Fred Thompson wants to have a grand entrance. But, of course, if you leave the audience sitting too long, the audience starts to get a little impatient. But the majority of opinion among those sampled at the state fair seems to be that they just want to learn more about Thompson. Mr. TERRY KATCH(ph) (Resident, Des Moines, Iowa): I know he's a TV actor and a former senator, but otherwise I don't know much about him. Ms. KAYE CHRISTIANSEN(ph) (Resident, Des Moines, Iowa): He needs to let us know who he is and what he's about and what his politics are. Mr. PAUL GUAN(ph) (Resident, Grimes, Iowa): You know, other than his movies and his TV, I didn't even realize until the last year or so that he was involved (unintelligible) back to Watergate. I didn't know him that well. Terry Katch and Kaye Christiansen of Des Moines and Paul Guan of Grimes, Iowa, were among the voters in line for the wildly unscientific corn kernel poll. Each added a kernel of corn to the (unintelligible) in jar labeled with the photo of their candidate of choice. Thompson's jars, for now, were half full. But it's not too late for him to make headway in the all-important Iowa caucuses in January, says David Yepsen, political columnist for the Des Moines Register. The challenge for - Senator Thompson is going to be finding the time to spend in Iowa, that he needs to spend to build an organization and still do everything else he has to do in this campaign like raise money and be in other early states. So far, Thompson's initial fundraising numbers for his testing-the-waters committee have fallen short of expectations. But Yepsen says there's still valuable manpower out there for the taking because Republican activists are still looking for someone they can embrace. Other Republican candidates have some flaw or limitation that causes problems with some of these activists. And Senator Thompson on - at the beginning seems to be a candidate who might be able to overcome some of those limitations. While Thompson spent the afternoon pressing the flesh here, he spent the morning meeting with state GOP legislators, social conservative groups and activists. And he used his speech at the fair to stake out his ground with conservative voters. I am unabashedly pro-life. I am pro-2nd amendment and I don't apologize for the United States of America. This country has shed more blood for the freedom of other people than all the other nations in the history of the world combined. And I'm tired of people feeling like they got to apologize for America. Fred Thompson is also not apologizing for holding off on declaring his candidacy. But this stance may not last for long. Thompson is widely expected to announce for president on September 5th, shortly after Labor Day. Audie Cornish, NPR News, Des Moines, Iowa.\n",
      "\n",
      "Reference Summary:\n",
      "Former senator and actor Fred Thompson, an undeclared Republican presidential hopeful, did not take part in the Aug. 11 Iowa straw poll. The state's caucuses begin the nominating process in January. Will Thompson's decision hurt him in the Hawkeye State if he officially enters the race?\n",
      "\n",
      "Generated Summary (Pre-trained BART):\n",
      "Fred Thompson says he's still testing the waters for a presidential run. But until yesterday, he hadn't dipped so much as a toe in Iowa. Mr. Thompson made a stop at the state fair in Des Moines on Friday, nearly a week after he skipped the Iowa Straw Poll. Republicans who are still not satisfied with their choices appear to be receptive to what the former senator from Tennessee has to say. NPR's Audie Cornish reports. Judging from the opening anecdotes at the speech at the Iowa state fair, actor and former Senator Fred Thompson knows \"Law & Order\" reruns aren't going to be enough to keep him afloat while he builds up to a presidential campaign. I was at an airport recently and a lady came up\n",
      "\n",
      "Generated Summary (Fine-tuned BART):\n",
      "Former Sen. Fred Thompson, a Tennessee Republican, says he's still testing the waters for a presidential run. But until Friday, he hadn't dipped so much as a toe in Iowa. Thompson made a stop at the state fair in Des Moines, nearly a week after he skipped the Iowa Straw Poll. Republicans who are still not satisfied with their choices appear to be receptive to what the former senator from Tennessee has to say.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Example 4:\n",
      "\n",
      "Original Text:\n",
      "This is DAY TO DAY, I'm Madeleine Brand. And I'm Alex Chadwick. Coming up, the story of a young New Hampshire war widow. First this. Mexico is electing it's President this Sunday. That race is gotten a lot of attention. And it's a nail biter. Opinion polls show the leading candidates neck and neck. They are conservative Felipe Calderon, and liberal Andres Manuel Lopez Obrador. A record 40 million people are expected to turn out to vote. As important for Mexico's political future is the Mexican Congressional Election. This weekend voters are going to elect all the members of both houses of congress. Michael O'Boyle reports from Mexico City. MICHAEL O'BOYLE reporting: It's the last meeting of congress before elections this Sunday. Lawmakers are debating which party is more guilty of manipulating welfare programs to buy votes. Sterile debates like this one help explain why Mexico's congress has been considered so inefficient. Vicente Fox's election in 2000 ended 70 years of one-party rule by the Institutional Revolution Party, or PRI. But while he won the presidency, the PRI has held on to more than a third of congressional seats. The leftist PRD has nearly a quarter. Fox's National Action Party, or PAN, holds slightly less than a third. During the last six years, the divided congress has been unwilling to approve any of Fox's major economic reforms. Lower house lawmaker Tomas Trueba from Fox's PAN says the congressional paralysis seen on economic issues is holding Mexico back. Mr. TOMAS TRUEBA (Lower House Member) (Through translation): These congressional elections are even more important than the presidential race. Because we have seen that it is in the congress where you have to reach the consensus needed to make this country governable. O'BOYLE: According to the polls, the new congress to be elected this Sunday is likely to end up just as dividend as the current one. So whoever wins the presidency probably won't have a legislative majority and will be forced to continue bargaining with the other parties in congress. The deadlock that dominated Fox's term has left many Mexicans frustrated with their lawmakers. O'BOYLE: Twenty-four-year-old Jose Garay(ph) sells clothes from his stand outside a subway stop. Mr. JOSE GARAY (Clothes Vendor) (Through translation): I would like to see the parties work together in coalition instead of fighting. We Mexicans have been waiting a long time for results and the parties need to work together for the good of the nation instead of just looking after their own interests. O'BOYLE: The goal of cooperation between the parties may be a long time coming. Roughly a third of Mexico's lawmakers are not directly elected but appointed by party leaders. That's stacks much of the congress with party apparatchiks. Furthermore, there is no re-election for any post in Mexico. That means lawmakers aren't very concerned with pleasing their constituents in order to win re-election. Rather, lawmakers want to satisfy their party leaders, who determine their next political appointment. Benito Nacif, an academic who studies congress at Mexico City's Cide University, says about 60 percent of Mexican lawmakers have never held a elected office in the past. The degree of professionalization within the Chamber of Deputies and the senate is very limited. O'BOYLE: Nacif thinks Mexico must allow for the re-election of its lawmakers as well as enacting other reforms to create a more professional and responsive legislature. In the meantime, the chances are slim that the next congress will be able to more forward on the major economic reforms that Mexico needs. Leftist Andres Manuel Lopez Obrador is leading polls to win the presidency this Sunday, but just barely. When he was mayor of Mexico City, his relationship with the opposition in the city assembly was conflicted. Some consider him stubborn and unwilling to negotiate. If that holds true, and he becomes president, Mexico could be headed for six more years of deadlock. For NPR News, I'm Michael O'Boyle in Mexico City. NPR News has more on the top presidential candidates in this election plus an analysis of the biggest issues that a new Mexican congress will have to face after the election. You can find all that at NPR.org.\n",
      "\n",
      "Reference Summary:\n",
      "Mexico’s presidential vote, set for Sunday, has the media’s full attention. But the congressional elections may be equally important. Many voters hope a new Congress can blunt the power of the presidency.\n",
      "\n",
      "Generated Summary (Pre-trained BART):\n",
      "This is DAY TO DAY, I'm Madeleine Brand. And I'm Alex Chadwick. Coming up, the story of a young New Hampshire war widow. First this. Mexico is electing it's President this Sunday. That race is gotten a lot of attention. And it's a nail biter. Opinion polls show the leading candidates neck and neck. They are conservative Felipe Calderon, and liberal Andres Manuel Lopez Obrador. A record 40 million people are expected to turn out to vote. As important for Mexico's political future is the Mexican Congressional Election. This weekend voters are going to elect all the members of both houses of congress. Michael O'Boyle reports from Mexico City. MICHAEL O'BO\n",
      "\n",
      "Generated Summary (Fine-tuned BART):\n",
      "Mexico's presidential election is on Sunday, and it's a nail-biter. Opinion polls show the leading candidates neck and neck between conservative Felipe Calderon and liberal Andres Manuel Lopez Obrador. A record 40 million people are expected to turn out to vote.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Example 5:\n",
      "\n",
      "Original Text:\n",
      "Good morning. I'm Renee Montagne. Earlier this week we reported some bad news for Led Zeppelin fans that came to us from Britain's Daily Mirror. To wit, lead singer Robert Plant tore up a multimillion-dollar contract to reunite with his band mates. The report had many feeling dazed and confused including, apparently, Robert Plant. The singer has not received any offers for a band reunion. The widely reported story was either made up, or a case of communication break-down. It's MORNING EDITION.\n",
      "\n",
      "Reference Summary:\n",
      "Earlier this week, we reported on a story that suggested Led Zeppelin had been offered a lot of money to reunite, but it appears there have been no offers for a band reunion.\n",
      "\n",
      "Generated Summary (Pre-trained BART):\n",
      "Good morning. I'm Renee Montagne. Earlier this week we reported some bad news for Led Zeppelin fans that came to us from Britain's Daily Mirror. To wit, lead singer Robert Plant tore up a multimillion-dollar contract to reunite with his band mates. The report had many feeling dazed and confused including, apparently, Robert Plant. The singer has not received any offers for a band reunion. The widely reported story was either made up, or a case of communication break-down. It's MORNING EDITION.\n",
      "\n",
      "Generated Summary (Fine-tuned BART):\n",
      "Earlier this week, we reported some bad news for Led Zeppelin fans. To wit, lead singer Robert Plant tore up a multimillion-dollar contract to reunite with his band mates. The report had many feeling dazed and confused, including Plant. The singer has not received any offers for a band reunion.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Example 6:\n",
      "\n",
      "Original Text:\n",
      "Since September the Louisiana Attorney General has been investigating allegations of mercy killings of patients at New Orleans hospitals in the days following Hurricane Katrina. Yesterday NPR reported for the first time that the investigation has amassed eyewitness accounts suggesting that patients may have died after being given lethal doses of painkillers. The investigation by the state's Attorney General has centered on the actions of doctors and nurses at New Orleans Memorial Hospital. NPR has reviewed secret court documents that reveal chilling details about events at Memorial Hospital in the chaotic days following the storm. NPR's Carrie Kahn has the story. Conditions at New Orleans Memorial Hospital after Katrina struck the city were horrendous. The building was flooded and temperatures inside soared past 100 degrees. But it was on the seventh floor where the situation was most dire. Memorial Hospital leased that floor to LifeCare Hospitals, a separate long-term patient care facility. A careful review of the court document shows that four eyewitnesses on the seventh floor believe that Memorial Hospital staff were preparing to euthanize patients. According to the documents, these witnesses said they heard hospital staff talk about the decision to end patients' lives and were told that lethal doses of painkillers would be administered. The documents say that attorneys for LifeCare reported all of this to the Louisiana State Attorney General's Office on September 14, 2005. One key discussion reported by witnesses took place Thursday morning, September 1st, three full days after the hurricane hit during what is described as an instant command meeting. A nurse told LifeCare's pharmacy director that the hospital's seventh floor LifeCare patients were critical and not expected to be evacuated with the rest of the hospital. Later that morning Dr. Anna Poe came up to the seventh floor. An actor reads from the documents. Witnesses' names have been removed. Dr. Poe told the witness that a decision had been made to administer lethal doses to these patients. The witness asked Poe what the lethal doses would be. The witness does not recall exactly what Dr. Poe said but believes it was morphine and Ativan. In the eyewitness accounts reviewed by NPR, LifeCare's pharmacy director says later that day he found Dr. Anna Poe and two unnamed nurses in the seventh floor medical charting room, according to a statement read here by an actor. Dr. Poe informed them that it had been decided that they were going to administer lethal doses to the LifeCare patients. From the documents reviewed by NPR it is not clear who gave that order. The witness asked Dr. Poe what medication was to be given. Again an actor reads from the documents. She showed him a big pack of vials of morphine and some loose vials also. Dr. Poe requested supplies such as syringes. While on the seventh floor another witness told the investigators she saw Dr. Poe drawing up medication. An actor reads from the investigator's account. The witness saw Dr. Poe walking down the hall accompanied by two white females who the witness assumed were nurses. Dr. Poe appeared to be nervous. The LifeCare staff were told to evacuate. Before they left the floor one witness said Dr. Poe had asked that they check on all the LifeCare patients and pull sheets over those who were deceased. LifeCare's pharmacy director told investigators that he saw Dr. Poe and two nurses enter the rooms where the remaining LifeCare patients were. No one has been charged in the investigation. And nowhere in the documents or in independent interviews conducted by NPR does anyone confirm seeing doctors or nurses administering lethal doses of morphine. Despite repeated phone calls and letters, Dr. Poe could not be reached for comment. Her lawyer, Rick Simmons, provided NPR with a written statement. It says, Dr. Poe and other medical personnel at Memorial Hospital worked tirelessly for five days to save and evacuate patients, none of whom were abandoned. When asked if Dr. Poe had euthanized any patients, Simmons told NPR that quote, \"Dr. Poe did not engage of any criminal actions.\" Tenet Healthcare Corporation, which owns Memorial Hospital, declined to comment on tape for this report. Tenet spokesman Harry Anderson said that evacuation plans for the seventh floor of Memorial were the sole responsibility of LifeCare Hospital. LifeCare Spokeswoman Paula Lavelle(ph) would not comment on the investigation but did stress that the company is cooperating fully with the Louisiana Attorney General. But in deference to the ongoing efforts of the AG's office and out of respect for the families of patients, we're unable to make any comment on matters related to the investigation. The New Orleans Coroner says the bodies were not retrieved from the hospital for two weeks after the storm. Because of that, he said, autopsies could not definitely determine the cause of death. The Attorney General, Charles Foti, says he cannot comment on the ongoing investigation. His office has subpoenaed more than 70 witnesses and is examining volumes of evidence. Carrie Kahn, NPR News.\n",
      "\n",
      "Reference Summary:\n",
      "Since September, the Louisiana Attorney General has been investigating allegations of mercy killings in hospitals in New Orleans. The investigation has amassed witness accounts suggesting patients at Memorial Hospital may have died from lethal doses of painkillers administered by medical staff in the days following Hurricane Katrina.\n",
      "\n",
      "Generated Summary (Pre-trained BART):\n",
      "Since September the Louisiana Attorney General has been investigating allegations of mercy killings of patients at New Orleans hospitals in the days following Hurricane Katrina. Yesterday NPR reported for the first time that the investigation has amassed eyewitness accounts suggesting that patients may have died after being given lethal doses of painkillers. The investigation by the state's Attorney General's Office has centered on the actions of doctors and nurses at New New Orleans Memorial Hospital. NPR has reviewed secret court documents that reveal chilling details about events at Memorial Hospital in the chaotic days following the storm. NPR's Carrie Kahn has the story. Conditions at New OR Memorial Hospital after Katrina struck the city were horrendous. The building was flooded and temperatures inside soared past 100 degrees. But it was on the seventh floor where the\n",
      "\n",
      "Generated Summary (Fine-tuned BART):\n",
      "The Louisiana Attorney General has been investigating allegations of mercy killings of patients at New Orleans Memorial Hospital in the days following Hurricane Katrina. On Wednesday, NPR reported for the first time that the investigation has amassed eyewitness accounts suggesting that patients may have died after being given lethal doses of painkillers.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Example 7:\n",
      "\n",
      "Original Text:\n",
      "President Trump recently pardoned Lewis \"Scooter\" Libby, the former chief of staff to Vice President Dick Cheney who was convicted of perjury and obstruction of justice in 2007. And yesterday, the president touted his power further, saying on Twitter that he was considering a posthumous pardon of heavyweight boxer Jack Johnson. Critics of the president have accused him of signaling that he may pardon his own associates, who are now under investigation. New York Attorney General Eric Schneiderman would like to have the power to prosecute any people the president may pardon. And so last week, he proposed to change the state's laws so that anyone pardoned will not be protected from being brought to court in New York. Schneiderman and President Trump have had business before. The New York AG led the investigation into Trump University. That resulted in a $25 million settlement. Trump, for his part, has called Mr. Schneiderman, quote, \"the nation's worst AG.\" But the proposal is complicated. And here to break it down for us - we're joined by former U.S. Attorney of Alabama Joyce Vance. Welcome. Hi, Lulu. What is the current New York law regarding prosecutions of people who've been pardoned for federal crimes? So New York law goes a little bit further than it has to go. The Fifth Amendment protects a defendant who's been tried on a charge once from being tried again. We call that double jeopardy. But it really only applies if you're tried by the same sovereign. There's a little loophole in New York law that apparently was unintended because it provides protection from being tried a second time not just for defendants who were acquitted the first time they were tried on a charge but also for defendants who received a pardon. So if this proposal passes, what changes? It's an issue of timing. So fast forwarding and putting this in the context of the Trump investigation, defendants who've already pleaded guilty or future defendants who might plead guilty or go to trial and have a jury impaneled in their case would then be ineligible for retrial in the state of New York if they were to be pardoned by President Trump at that point in the proceedings. And that technical timing issue is what's given prosecutors in the state of New York some concern. I guess the crux of the matter is, is there a specific reason, in your view, that Eric Schneiderman is bringing up this legislation? Who could President Trump potentially pardon that could end up under New York jurisdiction? Eric Schneiderman is being a good prosecutor and being extremely careful. They've obviously looked at the law on double jeopardy very carefully - and thinking that a defendant, perhaps Mr. Cohen, whose house and business were the subject of a federal search warrant over the past couple of weeks. And really anyone else who engaged in criminal activity in the state of New York could theoretically be prosecuted by the attorney general or by a district attorney. As long as I have you, I want to ask you about developments in the Mueller investigation. The president was having some trouble filling his legal team after John Dowd left. This week, Rudy Giuliani joined the team, saying he wants to bring an end to the Mueller investigation. Does that sound possible to you? It seems unlikely. Mr. Giuliani has said he expects that he will negotiate a settlement within the next couple of weeks. And among many other reasons, we know that's not possible because Paul Manafort faces two trials that won't take place until later this year. What does he bring to the team then? That's a good question. Obviously, the president has a comfort level with him. Mr. Giuliani is a former U.S. attorney, former leadership at the Justice Department. And one would hope that perhaps he can help the president understand that system a little better, as well. Joyce Vance is a former U.S. attorney of Alabama. Thank you so much. Thanks for having me.\n",
      "\n",
      "Reference Summary:\n",
      "NPR's Lulu Garcia-Navarro talks to former U.S. Attorney Joyce Vance about a proposal to change New York's criminal code to allow the state to prosecute people already granted presidential pardons.\n",
      "\n",
      "Generated Summary (Pre-trained BART):\n",
      "President Trump recently pardoned Lewis \"Scooter\" Libby, the former chief of staff to Vice President Dick Cheney who was convicted of perjury and obstruction of justice in 2007. And yesterday, the president touted his power further, saying on Twitter that he was considering a posthumous pardon of heavyweight boxer Jack Johnson. Critics of the president have accused him of signaling that he may pardon his own associates, who are now under investigation. New York Attorney General Eric Schneiderman would like to have the power to prosecute any people the president may pardon. And so last week, he proposed to change the state's laws so that anyone pardoned will not be protected from being brought to court in New York. Schneiderman and President Trump have had business\n",
      "\n",
      "Generated Summary (Fine-tuned BART):\n",
      "NPR's Lulu Garcia-Navarro speaks with Joyce Vance, former U.S. Attorney of Alabama, about President Trump's proposal to change New York law so that anyone pardoned will not be protected from being brought to court in New York.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Example 8:\n",
      "\n",
      "Original Text:\n",
      "Now a spoiler alert - if you are listening to our program with children who are expecting a certain visitor tonight, you might want to turn down the radio. Here's Ari with a poem. It's the night before Christmas, and here on our show, there's something we're really just dying to know. And because I'm speaking in holiday banter, you can guess that my question's related to Santa. And the question is this - for such a warm-hearted soul, why does he live in the chilly North Pole? With our answer is Fiona Halloran, who wrote a book about the guy who can be credited with placing Santa Claus at the North Pole. Who is this guy? Thomas Nast. Describe him. Thomas Nast is America's most famous political cartoonist. He was a powerful force in 19th century politics for a solid 20 years and then for some period after that. And he was a very deeply engaged participant in Republican politics, in particular. And in around the 1860s, he did some famous illustrations of Santa Claus. Describe what one of these cartoons looks like. Well, the first one, which many people have seen, shows Santa arriving in a Union Army camp at Christmas of 1863... This is, like, Civil War Union army. That's right - to provide gifts and Harper's Weekly newspapers to the soldiers. It's an advertisement. It's the cover of the magazine for which Nast worked, and so it's the children - the little drummer boys are excited by their copy of Harper's Weekly... (Laughter). ...Like you would be at Christmas. And how does the North Pole factor into these illustrations? Well, so, later - Thomas Nast eventually began to produce Christmas drawings every year because the public loved them. And so in later cartoons, he provided a wealth of detail, including in a couple of them an indication that Santa could be reached by mail at the North Pole. Talk about how people viewed the North Pole in the mid-1800s. What was it about this place that would have made it an appealing idea for people? Well, in the 1840s and 1850s, a number of explorers attempted to reach the North Pole. And people were interested in those efforts, in much the way that we are interested today in efforts to reach Mars, for example. And it's not, I think, accidental that this is the same time when people are traveling west to settle. And so there's a kind of adventurous, extreme quality to a lot of things that people were interested in. But it's clear that Santa had always been understood to be somebody associated with cold places, and the ultimate cold place that is remote is the North Pole. I think one of the appeals of the North Pole is that you couldn't get there, right? There's no danger that Santa's mystery will be erased because you're not going to run into him at the North Pole grocery store. (Laughter) Thomas Nast's cartoons also reshaped the way we view Santa as a physical presence. Talk about how his drawings changed our perception of this character. Well, in the earliest part of the 19th century, Christmas was a very different holiday in all kinds of ways. And one of the things that was different about it was that Santa was a much more forbidding character, and that reflected in his body. He was tall and slim. He looked like a bishop, and his job was to judge you. But in the middle part of the century, in general, Americans embraced sentimentality, a kind of domestic life that they idealized. And Thomas Nast helped to make Santa into a symbol of that by changing his body, and you find Santa getting tubbier, and his beard grows, and he suddenly has a pipe. And so, a lot of the things we think of as typically Santa - especially the sort of twinkly, grandfatherly quality of Santa - Nast really created that in his drawings. Fiona Halloran is the author of \"Thomas Nast: A Father Of Political Cartoons.\" Thanks for joining us. It was my pleasure.\n",
      "\n",
      "Reference Summary:\n",
      "NPR's Ari Shapiro interviews historian Fiona Halloran about the origins of placing Santa in the North Pole. She says it all began with Thomas Nast, a famous political cartoonist in the 19th century who completely redrew the image that Americans had of St. Nicholas.\n",
      "\n",
      "Generated Summary (Pre-trained BART):\n",
      "Now a spoiler alert - if you are listening to our program with children who are expecting a certain visitor tonight, you might want to turn down the radio. Here's Ari with a poem. It's the night before Christmas, and here on our show, there's something we're really just dying to know. And because I'm speaking in holiday banter, you can guess that my question's related to Santa. And the question is this - for such a warm-hearted soul, why does he live in the chilly North Pole? With our answer is Fiona Halloran, who wrote a book about the guy who can be credited with placing Santa Claus at the North Pole. Who is this guy? Thomas Nast. Describe him.\n",
      "\n",
      "Generated Summary (Fine-tuned BART):\n",
      "NPR's Ari Shapiro speaks with Fiona Halloran, author of The North Pole: A Memoir of Thomas Nast, a political cartoonist who is credited with placing Santa Claus at the North Pole.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Example 9:\n",
      "\n",
      "Original Text:\n",
      "Let's go to Brazil now, where protests that began last month have transformed the political landscape. Today, there is a national strike supported by several unions. Before all these demonstrations began, Brazil's President Dilma Rousseff seemed a sure bet to win reelection next year. Now her popularity has plummeted, and polls show she will probably face a runoff against another woman. Her name is Marina Silva. And as NPR's Lourdes Garcia-Navarro reports, she has a compelling rags-to-political-power story. As we walk into the hotel lobby, a man gets up to greet Marina Silva, saying hello, future president. Silva, a slight woman whose hair is coiled in a salt-and-pepper bun, nods graciously. Most politicians have been the big losers in Brazil's recent upheaval. Hundreds of thousands of people took to the streets here, decrying government corruption, among other complaints. But Marina Silva got a boost. She sat down with NPR for an extensive interview of Sao Paulo. (Through translator) I've been saying for a long time that we are seeing a new political awakening all around the world, and it's finding new ways of expressing itself. We are seeing an activism that is no longer directed by political parties. It is decentralized. It is leader-less. Leader-less it may be, but if the recent protest movement here signaled anything, it was a dissatisfaction with politics as usual. And Silva has benefited from that. She grew up poor, of Afro-Brazilian and Portuguese ancestry. Her father worked as a rubber tapper in the Amazon. She was orphaned young. She put herself through university working as a maid. She became politically active at a young age, too. She was a colleague of Chico Mendes, the union leader and environmentalist who was assassinated in the 1980s. A stint as a senator then led to a post as environment minister under former President Ignacio Lula da Silva. She quit, though, to join the Green Party, becoming their presidential candidate in 2010. She came in an impressive third place. Now she's forming her own party, with the aim of including many independents on her party's list. In Brazil, you have to be part of an acknowledged party to run for office, and that, she says, has excluded many groups from politics. (Through translator) We are proposing up to 30 percent of the party list to be available through independent candidates from civil society so people can have a place to defend their respective causes. Her message is also one of environmental activism. That's her background, after all. She speaks of the imminent collapse of civilization as we know it, unless things radically change. (Through translator) Brazil is a country that can make an important contribution to this world in crisis. We have the best conditions to switch to a sustainable model of consumption. We have 11 percent of the planet's freshwater, 20 percent of the living species of the planet, and we still have more than 200 indigenous tribes speaking more than 200 languages. In this new post-protest Brazil, her proposals resonate, says political analyst Marco Aurelio Nogueira from the University of Sao Paulo. (Through translator) These protests were anti-institutional, and she's always been seen as an outsider. So she is benefiting from the current political climate. But it's hard to know whether or not this popularity will last. Silva also appeals to a very powerful voting block here. She is an evangelical Christian, but her faith is a double-edged sword, says Nogueira. On the one hand, her environmental and political platform appeals to a more liberal base, but she is a social conservative. Silva treads very carefully when discussing her positions on controversial issues, like stem cell research - she's against - teaching creationism in the school - she's for having it included in school curriculum, along with evolution - and gay marriage. She says she is for everyone having the same civil rights. (Through translator) I wonder whether people would ask what role religion plays in my life if I was a Catholic. I am a person of faith, and I don't hide my faith. I don't use my faith as a political tool. When asked what her chances are to win the presidency next year, she demurs, saying she hasn't yet decided if she will run again. (Portuguese spoken) People ask me if I am an optimistic or a pessimist, she says. I say neither. I am only persistent. Lourdes Garcia-Navarro, NPR News, Sao Paulo.\n",
      "\n",
      "Reference Summary:\n",
      "Before national protests in Brazil, President Dilma Rousseff looked like she was guaranteed victory in next year's elections. Her popularity has since plummeted, and polls show she would face a run-off against Marina Silva, who grew up the daughter of a poor rubber tapper in the Amazon.\n",
      "\n",
      "Generated Summary (Pre-trained BART):\n",
      "Let's go to Brazil now, where protests that began last month have transformed the political landscape. Today, there is a national strike supported by several unions. Before all these demonstrations began, Brazil's President Dilma Rousseff seemed a sure bet to win reelection next year. Now her popularity has plummeted, and polls show she will probably face a runoff against another woman. Her name is Marina Silva. And as NPR's Lourdes Garcia-Navarro reports, she has a compelling rags-to-political-power story. As we walk into the hotel lobby, a man gets up to greet Marina Silva, saying hello, future president. Silva, a slight woman whose hair is coiled in a salt-and-pepper\n",
      "\n",
      "Generated Summary (Fine-tuned BART):\n",
      "Brazil's President Dilma Rousseff seemed a sure bet to win reelection next year. Now her popularity has plummeted, and polls show she will probably face a runoff against another woman. Her name is Marina Silva.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Example 10:\n",
      "\n",
      "Original Text:\n",
      "From NPR West, it's ALL THINGS CONSIDERED. I'm Arun Rath. We're going to begin the program today in South Sudan where despite talk of a possible cease-fire, the fighting continues. A power struggle there between the president and his former vice president spiraled into violence along tribal lines. Hundreds have died and tens of thousands are displaced. If not checked, many fear the conflict will become Africa's next civil war. And among the towns and cities most affected is Malakal in the country's north. NPR's Gregory Warner, who has been covering the conflict, reached Malakal today, and he joins us now on the line. Greg, can you describe what you've seen today? I'll try. You know, it's been really something. Malakal's been completely closed to outsiders since Christmas when the shooting really started. Today was the first day it was possible to get into the airport. As we were driving from the airport to the U.N. base here, where many people have camped out, there were dead bodies on the streets. All the shops are not only looted. They're just empty, burned-out shells. There's a smell of death in the air and a smell of smoke. There's some people who are burying their dead, but many people are just searching for food because there's just absolutely nothing. On the flip side, though, there is finally a sense of calm, and there were a fair number of people on the streets. Someone said to me, you know, these people have - they've seen war. They were - of course, South Sudan was at war for 21 years, so they know how to seize a moment of opportunity, whether that means grabbing a jerrican full of water from the Nile before the fighting resumes. And there is definitely a fear that the rebels have retreated, but the fighting is not over in Malakal. You traveled to Malakal today with the U.N. Are the U.N. peacekeepers able to bring any stability and relief supplies to the displaced? Well, you know, in Malakal, people sought shelter in the U.N. compound. And that happened in cities across South Sudan when the fighting started. Unlike, though, other U.N. compounds, the wall here is just not very high, so the bullets were whizzing through the compound. I met a 6-year-old who'd been shot in the stomach. She was inside the camp. In fact, her father said he didn't know whose bullet it was. Was it a government soldier's or was it anti-government soldier's? Now, of course, that there's some respite from the fighting, that the rebels have retreated for now, there is a massive humanitarian crisis. You know, Malakal is the second biggest city in South Sudan. It's the capital of the oil rich upper Nile. So all of a sudden, you have 10 to 20,000 people jammed into this compound. It's like a mini city in a place that has no running water, no latrines, a burgeoning cholera crisis, which if it spreads could make all the peacekeepers sick as well. And it really is chaos. People pitching tents into power lines. You know, the U.N. wants to tell these people to go, but the people say, well, we don't feel safe out there. We have to stay here. Now, the government offered a cease-fire yesterday and the prospects of talks with the rebels. Is there a possibility that initiative could gain momentum? Well, look. I mean, this dispute is fundamentally political, right? So there's two guys, President Salva Kiir, who's president of the country, and the former vice president Riek Machar. So, really, what both men want is power. And power in South Sudan is just centered in the hands of the president. The president has got all the power. So the fighting was triggered when Salva Kiir, either, you know, rightly or wrongly, depends on who you talk to, felt that some of his former ministers were plotting a coup against him. Now, of course, there's a lot of diplomatic pressure on these two men. But what they both want is the same thing. They both want to be president. Only one man can. So we hope that there could be some deal worked out between them. It's certainly possible with all the attention. But again, it's hard to resolve the fundamental kind of problem in South Sudan, which is it's a very young democracy. And this is the way power is won - through the gun. That's NPR's Gregory Warner in South Sudan. Greg, thank you and take care. Thanks Arun.\n",
      "\n",
      "Reference Summary:\n",
      "Heavy fighting in recent days between rebels and government-backed forces has created a wasteland in the South Sudan city of Malakal. Dead bodies line the streets and thousands of refugees are without food as U.N. peacekeepers struggle to assess the needs here. Host Arun Rath talks to NPR's Gregory Warner, who is traveling with U.N. officials in Malakal.\n",
      "\n",
      "Generated Summary (Pre-trained BART):\n",
      "From NPR West, it's ALL THINGS CONSIDERED. I'm Arun Rath. We're going to begin the program today in South Sudan where despite talk of a possible cease-fire, the fighting continues. A power struggle there between the president and his former vice president spiraled into violence along tribal lines. Hundreds have died and tens of thousands are displaced. If not checked, many fear the conflict will become Africa's next civil war. And among the towns and cities most affected is Malakal in the country's north. NPR's Gregory Warner, who has been covering the conflict, reached Malakals today, and he joins us now on the line. Greg, can you describe what you've seen today? I\n",
      "\n",
      "Generated Summary (Fine-tuned BART):\n",
      "Fighting continues in South Sudan's largest city, Malakal. The town is one of the towns and cities most affected by the fighting. NPR's Gregory Warner, who has been covering the conflict, speaks with Arun Rath about the situation there.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from evaluate import load\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Load the ROUGE metric\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "# Function to load the model and tokenizer\n",
    "def load_model(model_name=None, base_dir=\"./models\", pretrained_model=\"facebook/bart-base\"):\n",
    "    \"\"\"\n",
    "    Load a model and tokenizer. If model_name is None, load the pre-trained BART.\n",
    "    \"\"\"\n",
    "    if model_name:\n",
    "        model_path = os.path.join(base_dir, model_name)\n",
    "        model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "        tokenizer = BartTokenizer.from_pretrained(model_path)\n",
    "    else:\n",
    "        model = BartForConditionalGeneration.from_pretrained(pretrained_model)\n",
    "        tokenizer = BartTokenizer.from_pretrained(pretrained_model)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Function to load the test dataset\n",
    "def load_test_data(file_path):\n",
    "    \"\"\"\n",
    "    Load test data from a JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Function to generate a summary\n",
    "def generate_summary(model, tokenizer, text, max_length=150, min_length=40):\n",
    "    \"\"\"\n",
    "    Generate a summary for the given text using the model.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "        length_penalty=2.0,\n",
    "        num_beams=2,  # Reduce beams for faster generation\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Function to evaluate the model using ROUGE\n",
    "def evaluate_rouge(model, tokenizer, test_data, max_length=150, min_length=40):\n",
    "    \"\"\"\n",
    "    Evaluate the model using ROUGE metrics and return individual results.\n",
    "    \"\"\"\n",
    "    references = []\n",
    "    predictions = []\n",
    "    results = []\n",
    "    \n",
    "    for article in tqdm(test_data, desc=\"Evaluating\"):\n",
    "        original_text = article[\"text\"]\n",
    "        reference_summary = article[\"summary\"]\n",
    "        \n",
    "        # Generate the summary\n",
    "        generated_summary = generate_summary(model, tokenizer, original_text, max_length, min_length)\n",
    "        \n",
    "        # Append the generated and reference summaries\n",
    "        predictions.append(generated_summary)\n",
    "        references.append(reference_summary)\n",
    "        \n",
    "        # Save individual results\n",
    "        results.append({\n",
    "            \"Original Text\": original_text,\n",
    "            \"Reference Summary\": reference_summary,\n",
    "            \"Generated Summary\": generated_summary\n",
    "        })\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    return rouge_scores, pd.DataFrame(results)\n",
    "\n",
    "# Load test data\n",
    "test_file_path = \"./datasets/splits_filtered_with_summary/test.json\"  # Update path if necessary\n",
    "test_data = load_test_data(test_file_path)\n",
    "\n",
    "# Evaluate the pre-trained BART model\n",
    "print(\"\\nEvaluating Pre-trained BART...\")\n",
    "start_time = time.time()\n",
    "bart_model, bart_tokenizer = load_model()\n",
    "bart_model.to(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "bart_rouge_scores, bart_results_df = evaluate_rouge(bart_model, bart_tokenizer, test_data[:10])  # Use 10 examples for quick evaluation\n",
    "bart_end_time = time.time()\n",
    "\n",
    "# Evaluate your fine-tuned model\n",
    "print(\"\\nEvaluating Fine-tuned BART...\")\n",
    "start_time_ft = time.time()\n",
    "fine_tuned_model, fine_tuned_tokenizer = load_model(model_name=\"model_v3\")\n",
    "fine_tuned_model.to(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "ft_rouge_scores, ft_results_df = evaluate_rouge(fine_tuned_model, fine_tuned_tokenizer, test_data[:10])\n",
    "ft_end_time = time.time()\n",
    "\n",
    "# Display ROUGE scores comparison\n",
    "print(\"\\n=== ROUGE Scores Comparison ===\")\n",
    "rouge_comparison_df = pd.DataFrame({\n",
    "    \"Metric\": [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"ROUGE-Lsum\"],\n",
    "    \"Pre-trained BART\": [\n",
    "        bart_rouge_scores[\"rouge1\"],\n",
    "        bart_rouge_scores[\"rouge2\"],\n",
    "        bart_rouge_scores[\"rougeL\"],\n",
    "        bart_rouge_scores[\"rougeLsum\"]\n",
    "    ],\n",
    "    \"Fine-tuned BART\": [\n",
    "        ft_rouge_scores[\"rouge1\"],\n",
    "        ft_rouge_scores[\"rouge2\"],\n",
    "        ft_rouge_scores[\"rougeL\"],\n",
    "        ft_rouge_scores[\"rougeLsum\"]\n",
    "    ]\n",
    "})\n",
    "print(rouge_comparison_df.to_string(index=False, float_format=\"{:.4f}\".format))\n",
    "\n",
    "print(f\"\\nPre-trained BART Evaluation Time: {bart_end_time - start_time:.2f} seconds\")\n",
    "print(f\"Fine-tuned BART Evaluation Time: {ft_end_time - start_time_ft:.2f} seconds\")\n",
    "\n",
    "# Display individual examples\n",
    "print(\"\\n=== Sample Results Comparison ===\")\n",
    "for idx in range(len(bart_results_df)):\n",
    "    print(f\"\\nExample {idx + 1}:\")\n",
    "    print(\"\\nOriginal Text:\")\n",
    "    print(bart_results_df.iloc[idx][\"Original Text\"])\n",
    "    print(\"\\nReference Summary:\")\n",
    "    print(bart_results_df.iloc[idx][\"Reference Summary\"])\n",
    "    print(\"\\nGenerated Summary (Pre-trained BART):\")\n",
    "    print(bart_results_df.iloc[idx][\"Generated Summary\"])\n",
    "    print(\"\\nGenerated Summary (Fine-tuned BART):\")\n",
    "    print(ft_results_df.iloc[idx][\"Generated Summary\"])\n",
    "    print(\"\\n\" + \"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions from ROUGE Evaluation Results\n",
    "\n",
    "Based on the evaluation of both the pre-trained BART model and the fine-tuned BART model using ROUGE metrics, the following conclusions can be drawn:\n",
    "\n",
    "### ROUGE Score Comparison:\n",
    "\n",
    "1. **ROUGE-1** (unigram overlap):\n",
    "   - The pre-trained BART achieved a score of **0.3202**, while the fine-tuned BART reached **0.4465**.\n",
    "   - This marks an improvement of approximately **39%**, indicating significantly better word-level alignment with reference summaries after fine-tuning.\n",
    "\n",
    "2. **ROUGE-2** (bigram overlap):\n",
    "   - Pre-trained BART: **0.1250**, Fine-tuned BART: **0.2370**.\n",
    "   - Nearly a **90%** increase, which reflects a stronger ability of the fine-tuned model to capture local word sequences and context.\n",
    "\n",
    "3. **ROUGE-L** (longest common subsequence):\n",
    "   - Pre-trained BART: **0.2130**, Fine-tuned BART: **0.3610**.\n",
    "   - This represents a **69%** improvement, suggesting better structural coherence in the generated summaries.\n",
    "\n",
    "4. **ROUGE-Lsum** (summary-level variant of ROUGE-L):\n",
    "   - Pre-trained BART: **0.2138**, Fine-tuned BART: **0.3613**.\n",
    "   - These values closely mirror the ROUGE-L scores, confirming consistent gains in summary alignment.\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluation Time:\n",
    "\n",
    "1. **Pre-trained BART**:\n",
    "   - Evaluation time: **21.51 seconds**.\n",
    "\n",
    "2. **Fine-tuned BART**:\n",
    "   - Evaluation time: **7.99 seconds**, which is **2.7x faster**.\n",
    "\n",
    "> ⚠️ Note: The shorter generation time for the fine-tuned model may also be influenced by the model producing more concise outputs, not solely due to computational efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Improved Summary Generation Quality**:\n",
    "   - The fine-tuned BART model outperforms the pre-trained model across all ROUGE metrics, particularly in ROUGE-2 and ROUGE-L, demonstrating better contextual and structural summary generation.\n",
    "\n",
    "2. **Better Adaptation to Custom Data**:\n",
    "   - Fine-tuning enables the model to learn patterns specific to the user’s dataset, resulting in more relevant and coherent summaries.\n",
    "\n",
    "3. **Faster Inference Time**:\n",
    "   - The fine-tuned model produces summaries faster, which may be advantageous in real-time or resource-constrained applications.\n",
    "\n",
    "4. **Practical Implications**:\n",
    "   - Fine-tuned BART is more suitable for tasks requiring high-quality, domain-specific summaries, especially when the input data differs significantly from the general corpus used for pre-training.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "\n",
    "The fine-tuned BART model significantly outperforms the pre-trained version in terms of both output quality (as measured by ROUGE metrics) and generation speed. Fine-tuning proves to be a crucial step in tailoring the model to specific needs and improving its effectiveness in real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of BART Models: Pre-trained vs Fine-tuned - **BLEU**\n",
    "\n",
    "This code fragment compares the performance of a pre-trained BART model and a fine-tuned BART model on user data using the BLEU metric (Bilingual Evaluation Understudy). BLEU measures the similarity between generated summaries and reference summaries in the test set.\n",
    "\n",
    "---\n",
    "\n",
    "### Detailed Description:\n",
    "\n",
    "1. **Function `load_model`**:\n",
    "   - Loads the BART model and tokenizer from the specified directory.\n",
    "\n",
    "2. **Function `load_test_data`**:\n",
    "   - Loads test data from a JSON file.\n",
    "\n",
    "3. **Function `generate_summary`**:\n",
    "   - Generates a summary for a given input text using the BART model.\n",
    "   - Generation parameters:\n",
    "     - `max_length`: Maximum length of the summary (default is 150 tokens).\n",
    "     - `min_length`: Minimum length of the summary (default is 40 tokens).\n",
    "     - `num_beams=2`: Number of beams used during beam search generation.\n",
    "\n",
    "4. **Function `evaluate_bleu`**:\n",
    "   - Computes the BLEU score for the generated summaries.\n",
    "   - Steps:\n",
    "     - For each article in the test set:\n",
    "       - Generates a summary using the model.\n",
    "       - Compares the generated summary with the reference using NLTK's `sentence_bleu` function.\n",
    "     - Calculates the average BLEU score across all examples.\n",
    "\n",
    "5. **Evaluation Process**:\n",
    "   - Both models (pre-trained and fine-tuned) are evaluated on the same subset of test data (first 10 examples).\n",
    "   - BLEU scores and evaluation times for each model are printed in the console.\n",
    "\n",
    "---\n",
    "\n",
    "### Output:\n",
    "- The BLEU score for the pre-trained model (`Pre-trained BART BLEU`) and the fine-tuned model (`Fine-tuned BART BLEU`) is presented in decimal format.\n",
    "- The evaluation time for each model is also reported in seconds.\n",
    "\n",
    "---\n",
    "\n",
    "### Notes:\n",
    "- **BLEU score**:\n",
    "  - A higher BLEU score indicates greater similarity between generated and reference summaries.\n",
    "  - BLEU does not account for synonyms, so the results may not always perfectly reflect summary quality.\n",
    "- **Fine-tuned model performance**:\n",
    "  - The fine-tuned model is expected to achieve higher BLEU scores because it has been adapted to user-specific data.\n",
    "- **Use of a test subset**:\n",
    "  - To speed up evaluation, only 10 examples from the test set are used.\n",
    "- **Requirements**:\n",
    "  - NLTK must be installed (`pip install nltk`).\n",
    "  - Test data must include the `text` (original text) and `summary` (reference summary) fields.\n",
    "\n",
    "This code allows for comparing the quality of generated summaries between both models and assessing the impact of fine-tuning on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fine-tuned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU: 100%|██████████| 10/10 [00:06<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned BART BLEU: 0.1067\n",
      "Evaluation Time: 6.78 seconds\n",
      "\n",
      "Evaluating pre-trained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU: 100%|██████████| 10/10 [00:17<00:00,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained BART BLEU: 0.0411\n",
      "Evaluation Time: 17.91 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import time\n",
    "\n",
    "# Function to load the model and tokenizer\n",
    "def load_model(model_name, base_dir=\"./models\"):\n",
    "    \"\"\"\n",
    "    Load a pre-trained model and tokenizer from a directory.\n",
    "    \"\"\"\n",
    "    model_path = os.path.join(base_dir, model_name)\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Function to load the test dataset\n",
    "def load_test_data(file_path):\n",
    "    \"\"\"\n",
    "    Load test data from a JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Function to generate a summary\n",
    "def generate_summary(model, tokenizer, text, max_length=150, min_length=40):\n",
    "    \"\"\"\n",
    "    Generate a summary for the given text using the model.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "        length_penalty=2.0,\n",
    "        num_beams=2,  # Reduce beams for faster generation\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Function to evaluate the model using BLEU\n",
    "def evaluate_bleu(model, tokenizer, test_data, max_length=150, min_length=40):\n",
    "    \"\"\"\n",
    "    Evaluate the model using BLEU scores.\n",
    "    \"\"\"\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "    scores = []\n",
    "    \n",
    "    for article in tqdm(test_data, desc=\"Evaluating BLEU\"):\n",
    "        original_text = article[\"text\"]\n",
    "        reference_summary = article[\"summary\"]\n",
    "        \n",
    "        # Generate the summary\n",
    "        generated_summary = generate_summary(model, tokenizer, original_text, max_length, min_length)\n",
    "        \n",
    "        # Compute BLEU score\n",
    "        reference_tokens = [reference_summary.split()]\n",
    "        candidate_tokens = generated_summary.split()\n",
    "        score = sentence_bleu(reference_tokens, candidate_tokens, smoothing_function=smoothing_function)\n",
    "        scores.append(score)\n",
    "    \n",
    "    # Calculate average BLEU score\n",
    "    avg_bleu = sum(scores) / len(scores)\n",
    "    return avg_bleu\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"model_v3\"  # Replace with your fine-tuned model name\n",
    "pretrained_model_name = \"facebook/bart-base\"  # Pre-trained BART\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"  # Use MPS for Apple Silicon\n",
    "\n",
    "# Load fine-tuned model\n",
    "fine_tuned_model, fine_tuned_tokenizer = load_model(model_name)\n",
    "fine_tuned_model.to(device)\n",
    "\n",
    "# Load pre-trained model\n",
    "pretrained_model = BartForConditionalGeneration.from_pretrained(pretrained_model_name).to(device)\n",
    "pretrained_tokenizer = BartTokenizer.from_pretrained(pretrained_model_name)\n",
    "\n",
    "# Load test data\n",
    "test_file_path = \"./datasets/splits_filtered_with_summary/test.json\"  # Update path if necessary\n",
    "test_data = load_test_data(test_file_path)\n",
    "small_test_data = test_data[:10]  # Use only 10 examples for quick evaluation\n",
    "\n",
    "# Evaluate fine-tuned model\n",
    "print(\"Evaluating fine-tuned model...\")\n",
    "start_time = time.time()\n",
    "fine_tuned_bleu = evaluate_bleu(fine_tuned_model, fine_tuned_tokenizer, small_test_data)\n",
    "end_time = time.time()\n",
    "print(f\"Fine-tuned BART BLEU: {fine_tuned_bleu:.4f}\")\n",
    "print(f\"Evaluation Time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate pre-trained model\n",
    "print(\"\\nEvaluating pre-trained model...\")\n",
    "start_time = time.time()\n",
    "pretrained_bleu = evaluate_bleu(pretrained_model, pretrained_tokenizer, small_test_data)\n",
    "end_time = time.time()\n",
    "print(f\"Pre-trained BART BLEU: {pretrained_bleu:.4f}\")\n",
    "print(f\"Evaluation Time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions from BLEU Score Analysis\n",
    "\n",
    "Based on the evaluation results of the pre-trained and fine-tuned BART models using the BLEU metric, the following conclusions can be drawn:\n",
    "\n",
    "---\n",
    "\n",
    "### BLEU Scores:\n",
    "\n",
    "1. **Fine-tuned BART**:\n",
    "   - Achieved a BLEU score of **0.1067**.\n",
    "   - This indicates significantly better alignment between generated summaries and reference summaries compared to the pre-trained BART model.\n",
    "\n",
    "2. **Pre-trained BART**:\n",
    "   - Scored **0.0411** on the BLEU metric.\n",
    "   - This much lower score suggests that the pre-trained model produces summaries that are less accurate in the context of the user's specific dataset.\n",
    "\n",
    "3. **Difference in Scores**:\n",
    "   - The BLEU score of the fine-tuned BART is approximately **2.6 times higher** than that of the pre-trained BART.\n",
    "   - This difference highlights the importance of fine-tuning in improving the quality of generated summaries, especially when test data differs from the generic corpora used to train the base model.\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluation Time:\n",
    "\n",
    "1. **Fine-tuned BART**:\n",
    "   - The evaluation took **6.78 seconds**, significantly faster than the pre-trained model.\n",
    "\n",
    "2. **Pre-trained BART**:\n",
    "   - The evaluation took **17.91 seconds**, which is approximately **2.64 times longer** than the fine-tuned model.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Improved Summary Quality**:\n",
    "   - Fine-tuned BART generates summaries that are more consistent with reference summaries, as reflected in its higher BLEU score.\n",
    "   - This suggests the model is better at capturing key linguistic and semantic patterns in the user-specific dataset.\n",
    "\n",
    "2. **Better Time Efficiency**:\n",
    "   - In addition to producing more accurate summaries, the fine-tuned BART does so in significantly less time, making it more suitable for real-time or production environments.\n",
    "\n",
    "3. **Value of Fine-tuning**:\n",
    "   - The substantial difference in BLEU scores between the pre-trained and fine-tuned models emphasizes the effectiveness of the fine-tuning process in enhancing model performance for domain-specific tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "\n",
    "The fine-tuned BART model clearly outperforms the pre-trained BART in both summary quality and processing speed. Fine-tuning proves essential in adapting the model to the user's specific needs, resulting in more accurate and coherent summaries delivered in less time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
