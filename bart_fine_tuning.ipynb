{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konfiguracja środowiska do trenowania modelu BART\n",
    "\n",
    "Ten fragment kodu przygotowuje środowisko do trenowania modelu BART, wykorzystując PyTorch oraz bibliotekę Transformers. Obejmuje konfigurację urządzenia, załadowanie plików danych oraz ustawienie parametrów globalnych.\n",
    "\n",
    "### Szczegółowy opis:\n",
    "\n",
    "1. **Sprawdzenie wsparcia dla Metal Performance Shaders (MPS)**:\n",
    "   - Na macOS sprawdzana jest dostępność MPS, co pozwala na przyspieszenie obliczeń na procesorach Apple Silicon.\n",
    "   - Jeśli MPS nie jest dostępny, kod używa CPU.\n",
    "\n",
    "2. **Globalna konfiguracja (`CONFIG`)**:\n",
    "   - Słownik `CONFIG` definiuje kluczowe parametry trenowania:\n",
    "     - `train_file`, `val_file`, `test_file`: Ścieżki do plików JSON zawierających dane podzielone na zbiory treningowe, walidacyjne i testowe.\n",
    "     - `batch_size`: Rozmiar partii danych w DataLoaderze (6 próbek na partię).\n",
    "     - `epochs`: Liczba epok trenowania (5 iteracji przez cały zbiór treningowy).\n",
    "     - `learning_rate`: Współczynnik uczenia dla optymalizatora AdamW.\n",
    "     - `max_length`: Maksymalna długość tokenów po tokenizacji (512 tokenów).\n",
    "     - `seed`: Ziarno losowości dla powtarzalności wyników.\n",
    "\n",
    "3. **Ustawienie ziarna losowości**:\n",
    "   - Kod ustawia ziarno losowości dla modułów `random` i `torch`, co zapewnia powtarzalność wyników podczas trenowania i generowania danych.\n",
    "\n",
    "4. **Urządzenie obliczeniowe**:\n",
    "   - Informacja o używanym urządzeniu jest drukowana w konsoli (`Using device: mps` lub `Using device: cpu`).\n",
    "\n",
    "### Uwagi:\n",
    "- Wymagana jest instalacja biblioteki Transformers (`pip install transformers`) oraz PyTorch z odpowiednim wsparciem dla MPS lub CPU.\n",
    "- Globalna konfiguracja ułatwia szybkie dostosowanie parametrów trenowania bez konieczności modyfikowania kodu.\n",
    "- Ziarno losowości gwarantuje, że wyniki będą takie same przy każdym uruchomieniu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x131c34510>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check for Metal Performance Shaders (MPS) support on macOS\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Global configuration dictionary\n",
    "CONFIG = {\n",
    "    \"train_file\": \"./datasets/splits_filtered_with_summary/train.json\",\n",
    "    \"val_file\": \"./datasets/splits_filtered_with_summary/val.json\",\n",
    "    \"test_file\": \"./datasets/splits_filtered_with_summary/test.json\",\n",
    "    \"batch_size\": 6,  # Batch size for DataLoader\n",
    "    \"epochs\": 5,  # Number of training epochs\n",
    "    \"learning_rate\": 3e-5,  # Learning rate for the optimizer\n",
    "    \"max_length\": 512,  # Maximum sequence length for tokenization\n",
    "    \"seed\": 42  # Random seed for reproducibility\n",
    "}\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "random.seed(CONFIG[\"seed\"])\n",
    "torch.manual_seed(CONFIG[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie i tokenizacja danych do trenowania modelu BART\n",
    "\n",
    "Ten fragment kodu wczytuje dane z plików JSON, tokenizuje je i tworzy dedykowany zbiór danych (`Dataset`) dla modelu BART. Zbiory treningowe, walidacyjne i testowe są następnie przygotowane do użycia w procesie trenowania.\n",
    "\n",
    "### Szczegółowy opis:\n",
    "\n",
    "1. **Wczytanie danych z plików JSON**:\n",
    "   - Funkcja `load_json_data(file_path)` wczytuje dane z podanego pliku JSON i zwraca listę słowników zawierających `id`, `text`, oraz `summary`.\n",
    "   - Dane wczytywane są z plików określonych w konfiguracji `CONFIG`:\n",
    "     - `CONFIG[\"train_file\"]`: Plik ze zbiorem treningowym.\n",
    "     - `CONFIG[\"val_file\"]`: Plik ze zbiorem walidacyjnym.\n",
    "     - `CONFIG[\"test_file\"]`: Plik ze zbiorem testowym.\n",
    "\n",
    "2. **Klasa `TextDataset`**:\n",
    "   - Dedykowana klasa `Dataset` oparta na PyTorch do obsługi danych tokenizowanych.\n",
    "   - Argumenty:\n",
    "     - `data`: Lista danych z kluczami `text` i `summary`.\n",
    "     - `tokenizer`: Tokenizer BART-a (`facebook/bart-base`).\n",
    "     - `max_length`: Maksymalna długość tokenów (określona w `CONFIG`).\n",
    "   - Metody:\n",
    "     - `__len__`: Zwraca liczbę przykładów w zbiorze danych.\n",
    "     - `__getitem__`: Tokenizuje dane wejściowe (`text`) i dane docelowe (`summary`) dla konkretnego przykładu. Zwraca:\n",
    "       - `input_ids`: Tokeny wejściowe.\n",
    "       - `attention_mask`: Maska uwagi dla tokenów wejściowych.\n",
    "       - `labels`: Tokeny docelowe dla streszczenia.\n",
    "\n",
    "3. **Tworzenie zbiorów danych**:\n",
    "   - Zbiory treningowe, walidacyjne i testowe są tworzone przy użyciu klasy `TextDataset`.\n",
    "   - Parametry tokenizera, takie jak `max_length`, są zgodne z globalną konfiguracją.\n",
    "\n",
    "4. **Podsumowanie zbiorów danych**:\n",
    "   - Drukowana jest liczba przykładów w każdym zbiorze (`train`, `val`, `test`).\n",
    "\n",
    "### Wynik:\n",
    "- Dane wejściowe i docelowe są odpowiednio tokenizowane i przygotowane w formacie kompatybilnym z PyTorch DataLoader.\n",
    "- Kod umożliwia łatwe rozszerzenie na inne modele i konfiguracje, zapewniając skalowalność.\n",
    "\n",
    "### Uwagi:\n",
    "- Tokenizer BART-a musi być załadowany za pomocą biblioteki Hugging Face Transformers (`pip install transformers`).\n",
    "- Zbiory danych można łatwo używać w procesie trenowania z wykorzystaniem PyTorch.\n",
    "- Klasa `TextDataset` zapewnia czytelność i modularność kodu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 14815\n",
      "Validation examples: 7407\n",
      "Test examples: 2470\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "# Load the tokenizer for BART\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "# Function to load JSON data from a file\n",
    "def load_json_data(file_path):\n",
    "    \"\"\"\n",
    "    Load JSON data from a file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries with \"id\", \"text\", and \"summary\".\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Load train, validation, and test data\n",
    "train_data = load_json_data(CONFIG[\"train_file\"])\n",
    "val_data = load_json_data(CONFIG[\"val_file\"])\n",
    "test_data = load_json_data(CONFIG[\"test_file\"])\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset to handle tokenized input for BART fine-tuning.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with tokenized data.\n",
    "\n",
    "        Args:\n",
    "            data (list): List of dictionaries with \"text\" and \"summary\".\n",
    "            tokenizer (BartTokenizer): Tokenizer for BART.\n",
    "            max_length (int): Maximum sequence length for tokenization.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the size of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Tokenize a specific example and prepare input/output tensors.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the example.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing input_ids, attention_mask, and labels.\n",
    "        \"\"\"\n",
    "        entry = self.data[idx]\n",
    "        input_text = entry[\"text\"]\n",
    "        target_text = entry[\"summary\"]\n",
    "\n",
    "        # Tokenize the input text\n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Tokenize the target text\n",
    "        targets = self.tokenizer(\n",
    "            target_text,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": targets[\"input_ids\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "# Create datasets for train, validation, and test\n",
    "train_dataset = TextDataset(train_data, tokenizer, CONFIG[\"max_length\"])\n",
    "val_dataset = TextDataset(val_data, tokenizer, CONFIG[\"max_length\"])\n",
    "test_dataset = TextDataset(test_data, tokenizer, CONFIG[\"max_length\"])\n",
    "\n",
    "# Print a summary of the datasets\n",
    "print(f\"Training examples: {len(train_dataset)}\")\n",
    "print(f\"Validation examples: {len(val_dataset)}\")\n",
    "print(f\"Test examples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie DataLoaderów\n",
    "\n",
    "Ten fragment kodu tworzy DataLoadery dla zbiorów treningowego, walidacyjnego i testowego, umożliwiając efektywne przetwarzanie danych w partiach podczas trenowania i ewaluacji modelu BART.\n",
    "\n",
    "### Szczegółowy opis:\n",
    "\n",
    "1. **Funkcja `prepare_data_loaders`**:\n",
    "   - Tworzy DataLoadery dla podanych zbiorów danych.\n",
    "   - Argumenty:\n",
    "     - `train_dataset`: Zbiór danych treningowych.\n",
    "     - `val_dataset`: Zbiór danych walidacyjnych.\n",
    "     - `test_dataset`: Zbiór danych testowych.\n",
    "     - `batch_size`: Liczba przykładów w jednej partii (batch).\n",
    "   - Zwraca krotkę z DataLoaderami dla zbiorów:\n",
    "     - `train_loader`: DataLoader z danymi treningowymi (shuffle=True dla losowego porządku).\n",
    "     - `val_loader`: DataLoader z danymi walidacyjnymi (shuffle=False, dane w określonej kolejności).\n",
    "     - `test_loader`: DataLoader z danymi testowymi (shuffle=False).\n",
    "\n",
    "2. **Przygotowanie DataLoaderów**:\n",
    "   - DataLoadery są tworzone z wykorzystaniem klasy `DataLoader` z PyTorch.\n",
    "   - Rozmiar partii jest definiowany w konfiguracji `CONFIG[\"batch_size\"]`.\n",
    "\n",
    "3. **Podsumowanie DataLoaderów**:\n",
    "   - Wyświetlana jest liczba partii danych dla każdego zbioru:\n",
    "     - `Train DataLoader batches`: Liczba partii w zbiorze treningowym.\n",
    "     - `Validation DataLoader batches`: Liczba partii w zbiorze walidacyjnym.\n",
    "     - `Test DataLoader batches`: Liczba partii w zbiorze testowym.\n",
    "\n",
    "### Wynik:\n",
    "- DataLoadery są gotowe do użycia w procesie trenowania i ewaluacji modelu.\n",
    "- Liczba partii w każdym DataLoaderze zależy od liczby przykładów w zbiorze danych i rozmiaru partii (`batch_size`).\n",
    "\n",
    "### Uwagi:\n",
    "- DataLoadery automatycznie ładują dane w partiach, co poprawia wydajność trenowania.\n",
    "- Ustawienie `shuffle=True` w zbiorze treningowym pozwala na losowe wybieranie danych, co zapobiega problemowi nadmiernego dopasowania (overfitting).\n",
    "- Funkcja jest modularna i może być ponownie wykorzystana w innych projektach NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataLoader batches: 2470\n",
      "Validation DataLoader batches: 1235\n",
      "Test DataLoader batches: 412\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def prepare_data_loaders(train_dataset, val_dataset, test_dataset, batch_size):\n",
    "    \"\"\"\n",
    "    Prepare DataLoaders for train, validation, and test datasets.\n",
    "\n",
    "    Args:\n",
    "        train_dataset (Dataset): Dataset for training.\n",
    "        val_dataset (Dataset): Dataset for validation.\n",
    "        test_dataset (Dataset): Dataset for testing.\n",
    "        batch_size (int): Batch size for DataLoaders.\n",
    "\n",
    "    Returns:\n",
    "        tuple: DataLoaders for train, validation, and test datasets.\n",
    "    \"\"\"\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Prepare DataLoaders\n",
    "train_loader, val_loader, test_loader = prepare_data_loaders(\n",
    "    train_dataset, val_dataset, test_dataset, CONFIG[\"batch_size\"]\n",
    ")\n",
    "\n",
    "# Print a summary of DataLoaders\n",
    "print(f\"Train DataLoader batches: {len(train_loader)}\")\n",
    "print(f\"Validation DataLoader batches: {len(val_loader)}\")\n",
    "print(f\"Test DataLoader batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicjalizacja modelu BART i optymalizatora\n",
    "\n",
    "Ten fragment kodu ładuje wstępnie wytrenowany model BART, przenosi go na odpowiednie urządzenie obliczeniowe i definiuje optymalizator do trenowania modelu.\n",
    "\n",
    "### Szczegółowy opis:\n",
    "\n",
    "1. **Ładowanie modelu BART**:\n",
    "   - Model `BartForConditionalGeneration` jest wczytywany z pretrenowanego modelu `facebook/bart-base` z biblioteki Hugging Face Transformers.\n",
    "   - Ten model jest przeznaczony do generacji tekstu, np. streszczeń.\n",
    "\n",
    "2. **Przenoszenie modelu na urządzenie**:\n",
    "   - Model jest przenoszony na urządzenie określone w zmiennej `device`:\n",
    "     - MPS (dla macOS z Apple Silicon) lub CPU, jeśli MPS nie jest dostępne.\n",
    "   - To umożliwia wykorzystanie odpowiedniego sprzętu do obliczeń.\n",
    "\n",
    "3. **Definiowanie optymalizatora**:\n",
    "   - Optymalizator `AdamW` (optymalizator Adam z regularizacją wag) jest tworzony do aktualizacji wag modelu.\n",
    "   - Współczynnik uczenia (`learning_rate`) jest ustawiany zgodnie z wartością z konfiguracji `CONFIG[\"learning_rate\"]`.\n",
    "\n",
    "4. **Podsumowanie**:\n",
    "   - Wyświetlany jest komunikat potwierdzający załadowanie modelu i przeniesienie go na urządzenie.\n",
    "\n",
    "### Wynik:\n",
    "- Model BART jest gotowy do trenowania lub ewaluacji.\n",
    "- Optymalizator został skonfigurowany do aktualizowania parametrów modelu podczas procesu trenowania.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and moved to device.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Load the pre-trained BART model\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "\n",
    "# Print model summary\n",
    "print(\"Model loaded and moved to device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walidacja zbiorów danych\n",
    "\n",
    "Ten fragment kodu waliduje zbiory danych (treningowy, walidacyjny, testowy), aby upewnić się, że wszystkie wpisy zawierają wymagane klucze `\"text\"` i `\"summary\"`. \n",
    "\n",
    "### Szczegółowy opis:\n",
    "\n",
    "1. **Funkcja `validate_dataset`**:\n",
    "   - Przeszukuje listę słowników (`data`) i sprawdza, czy każdy wpis zawiera klucze `\"text\"` i `\"summary\"`.\n",
    "   - Argumenty:\n",
    "     - `data`: Lista słowników do walidacji.\n",
    "   - Zwraca:\n",
    "     - Listę nieprawidłowych wpisów w formacie `(index, entry)`, gdzie:\n",
    "       - `index`: Indeks wpisu w liście.\n",
    "       - `entry`: Treść wpisu.\n",
    "\n",
    "2. **Walidacja zbiorów danych**:\n",
    "   - Funkcja `validate_dataset` jest wywoływana dla każdego ze zbiorów:\n",
    "     - `train_data` (zbiór treningowy).\n",
    "     - `val_data` (zbiór walidacyjny).\n",
    "     - `test_data` (zbiór testowy).\n",
    "   - Wynik walidacji to liczba nieprawidłowych wpisów w każdym zbiorze, wyświetlana w konsoli.\n",
    "\n",
    "3. **Wyjście**:\n",
    "   - Liczba wpisów bez wymaganych kluczy (`\"text\"` i `\"summary\"`) jest drukowana dla każdego zbioru danych:\n",
    "     - `Invalid entries in train dataset`.\n",
    "     - `Invalid entries in validation dataset`.\n",
    "     - `Invalid entries in test dataset`.\n",
    "\n",
    "### Wynik:\n",
    "- Jeśli wszystkie wpisy w zbiorach danych są prawidłowe, funkcja zwraca puste listy dla każdego zbioru.\n",
    "- Jeśli występują nieprawidłowe wpisy, użytkownik otrzymuje ich liczbę oraz szczegóły (indeksy i zawartość).\n",
    "\n",
    "### Uwagi:\n",
    "- Walidacja jest kluczowa przed przystąpieniem do trenowania modelu, aby uniknąć błędów w procesie tokenizacji i generacji tekstu.\n",
    "- Funkcja jest uniwersalna i może być używana do walidacji innych zbiorów danych w formacie listy słowników."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating train dataset...\n",
      "Invalid entries in train dataset: 0\n",
      "Validating validation dataset...\n",
      "Invalid entries in validation dataset: 0\n",
      "Validating test dataset...\n",
      "Invalid entries in test dataset: 0\n"
     ]
    }
   ],
   "source": [
    "def validate_dataset(data):\n",
    "    \"\"\"\n",
    "    Validates the dataset to ensure all entries contain \"text\" and \"summary\".\n",
    "\n",
    "    Args:\n",
    "        data (list): List of dictionaries to validate.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of invalid entries with their indices.\n",
    "    \"\"\"\n",
    "    invalid_entries = []\n",
    "    for idx, entry in enumerate(data):\n",
    "        if \"text\" not in entry or \"summary\" not in entry:\n",
    "            invalid_entries.append((idx, entry))\n",
    "    return invalid_entries\n",
    "\n",
    "# Validate train, validation, and test datasets\n",
    "print(\"Validating train dataset...\")\n",
    "invalid_train = validate_dataset(train_data)\n",
    "print(f\"Invalid entries in train dataset: {len(invalid_train)}\")\n",
    "\n",
    "print(\"Validating validation dataset...\")\n",
    "invalid_val = validate_dataset(val_data)\n",
    "print(f\"Invalid entries in validation dataset: {len(invalid_val)}\")\n",
    "\n",
    "print(\"Validating test dataset...\")\n",
    "invalid_test = validate_dataset(test_data)\n",
    "print(f\"Invalid entries in test dataset: {len(invalid_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspekcja DataLoaderów\n",
    "\n",
    "Ten fragment kodu inspektuje DataLoadery dla zbiorów treningowego, walidacyjnego i testowego, aby upewnić się, że są poprawnie skonfigurowane i zawierają dane w odpowiednim formacie.\n",
    "\n",
    "### Szczegółowy opis:\n",
    "\n",
    "1. **Funkcja `inspect_data_loaders`**:\n",
    "   - Inspektuje podany `DataLoader`, wyświetlając szczegóły dotyczące pierwszej partii danych.\n",
    "   - Argumenty:\n",
    "     - `loader`: DataLoader, który ma być inspektowany.\n",
    "     - `loader_name`: Nazwa DataLoadera, używana do logowania informacji.\n",
    "   - Funkcja wyświetla:\n",
    "     - Klucze w partii danych (`input_ids`, `attention_mask`, `labels`).\n",
    "     - Kształt (`shape`) każdej składowej partii danych.\n",
    "   - Obsługuje potencjalne błędy, aby zapobiec przerwaniu działania w przypadku nieprawidłowego DataLoadera.\n",
    "\n",
    "2. **Inspekcja zbiorów danych**:\n",
    "   - Inspekcja jest przeprowadzana dla trzech DataLoaderów:\n",
    "     - `train_loader`: Zbiór treningowy.\n",
    "     - `val_loader`: Zbiór walidacyjny.\n",
    "     - `test_loader`: Zbiór testowy.\n",
    "   - Dla każdego DataLoadera drukowane są szczegóły pierwszej partii.\n",
    "\n",
    "3. **Wyjście**:\n",
    "   - Wyświetlane są klucze partii danych, np.:\n",
    "     - `input_ids`: Tokeny wejściowe.\n",
    "     - `attention_mask`: Maska uwagi dla tokenów wejściowych.\n",
    "     - `labels`: Tokeny docelowe.\n",
    "   - Wyświetlany jest kształt danych dla każdego klucza, co pozwala zweryfikować, czy dane są odpowiednio przygotowane.\n",
    "\n",
    "### Wynik:\n",
    "- Jeśli DataLoadery są poprawnie skonfigurowane, użytkownik zobaczy szczegóły pierwszej partii dla każdego DataLoadera.\n",
    "- Jeśli występują błędy w danych lub konfiguracji DataLoadera, zostaną wyświetlone odpowiednie komunikaty o błędach.\n",
    "\n",
    "### Uwagi:\n",
    "- Inspekcja DataLoaderów jest kluczowym krokiem w weryfikacji poprawności konfiguracji przed rozpoczęciem trenowania modelu.\n",
    "- Funkcja może być rozszerzona o dodatkowe sprawdzenia, takie jak wartości danych (np. minimalne/maksymalne wartości tokenów).\n",
    "- Umożliwia szybkie zidentyfikowanie problemów, takich jak brakujące klucze lub nieprawidłowe kształty tensorów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspecting Train DataLoader...\n",
      "Batch keys in Train DataLoader: ['input_ids', 'attention_mask', 'labels']\n",
      "Sample 'input_ids' shape: torch.Size([6, 512])\n",
      "Sample 'attention_mask' shape: torch.Size([6, 512])\n",
      "Sample 'labels' shape: torch.Size([6, 512])\n",
      "\n",
      "Inspecting Validation DataLoader...\n",
      "Batch keys in Validation DataLoader: ['input_ids', 'attention_mask', 'labels']\n",
      "Sample 'input_ids' shape: torch.Size([6, 512])\n",
      "Sample 'attention_mask' shape: torch.Size([6, 512])\n",
      "Sample 'labels' shape: torch.Size([6, 512])\n",
      "\n",
      "Inspecting Test DataLoader...\n",
      "Batch keys in Test DataLoader: ['input_ids', 'attention_mask', 'labels']\n",
      "Sample 'input_ids' shape: torch.Size([6, 512])\n",
      "Sample 'attention_mask' shape: torch.Size([6, 512])\n",
      "Sample 'labels' shape: torch.Size([6, 512])\n"
     ]
    }
   ],
   "source": [
    "# Inspect DataLoaders to ensure they are correctly set up\n",
    "def inspect_data_loaders(loader, loader_name):\n",
    "    \"\"\"\n",
    "    Inspects a DataLoader by printing a summary of the first batch.\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader): DataLoader to inspect.\n",
    "        loader_name (str): Name of the DataLoader for logging purposes.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(f\"\\nInspecting {loader_name}...\")\n",
    "    try:\n",
    "        # Get the first batch\n",
    "        for batch in loader:\n",
    "            print(f\"Batch keys in {loader_name}: {list(batch.keys())}\")\n",
    "            print(f\"Sample 'input_ids' shape: {batch['input_ids'].shape}\")\n",
    "            print(f\"Sample 'attention_mask' shape: {batch['attention_mask'].shape}\")\n",
    "            print(f\"Sample 'labels' shape: {batch['labels'].shape}\")\n",
    "            break  # Stop after the first batch\n",
    "    except Exception as e:\n",
    "        print(f\"Error inspecting {loader_name}: {e}\")\n",
    "\n",
    "# Inspect the train, validation, and test DataLoaders\n",
    "inspect_data_loaders(train_loader, \"Train DataLoader\")\n",
    "inspect_data_loaders(val_loader, \"Validation DataLoader\")\n",
    "inspect_data_loaders(test_loader, \"Test DataLoader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wyświetlanie losowych przykładów ze zbiorów danych\n",
    "\n",
    "Ten fragment kodu wczytuje dane z plików JSON i wyświetla losowy przykład z każdego zbioru danych (treningowego, walidacyjnego i testowego). Pomaga to szybko zweryfikować zawartość danych w każdym zbiorze.\n",
    "\n",
    "### Szczegółowy opis:\n",
    "\n",
    "1. **Funkcja `display_random_sample`**:\n",
    "   - Wyświetla losowy przykład ze zbioru danych.\n",
    "   - Argumenty:\n",
    "     - `file_path`: Ścieżka do pliku JSON zawierającego zbiór danych.\n",
    "     - `dataset_name`: Nazwa zbioru danych, używana w komunikatach (np. \"Train\", \"Validation\", \"Test\").\n",
    "   - Kroki:\n",
    "     - Wczytuje dane z pliku JSON.\n",
    "     - Sprawdza, czy zbiór danych nie jest pusty.\n",
    "     - Wybiera losowy przykład i wyświetla:\n",
    "       - `ID`: Unikalny identyfikator przykładu.\n",
    "       - `Text`: Treść artykułu.\n",
    "       - `Summary`: Streszczenie artykułu.\n",
    "\n",
    "2. **Obsługa błędów**:\n",
    "   - Sprawdza, czy plik istnieje. Jeśli plik nie zostanie znaleziony, wyświetla odpowiedni komunikat.\n",
    "   - Obsługuje inne potencjalne błędy (np. problemy z odczytem pliku) i wyświetla informacje o błędzie.\n",
    "\n",
    "3. **Wyświetlanie przykładów**:\n",
    "   - Funkcja jest wywoływana dla trzech zbiorów danych:\n",
    "     - `train.json` (zbiór treningowy).\n",
    "     - `val.json` (zbiór walidacyjny).\n",
    "     - `test.json` (zbiór testowy).\n",
    "   - Wyświetlane są szczegóły losowo wybranego przykładu z każdego zbioru.\n",
    "\n",
    "### Wynik:\n",
    "- Dla każdego zbioru danych wyświetlany jest losowy przykład, który zawiera:\n",
    "  - `ID`: Unikalny identyfikator przykładu.\n",
    "  - `Text`: Treść artykułu (przykładowy tekst).\n",
    "  - `Summary`: Streszczenie artykułu.\n",
    "- Przykłady są oddzielone poziomą linią (`-`).\n",
    "\n",
    "### Uwagi:\n",
    "- Funkcja pozwala szybko zweryfikować poprawność i zawartość danych w każdym zbiorze.\n",
    "- Jeśli zbiór danych jest pusty, wyświetlany jest komunikat informujący o braku danych.\n",
    "- Kod jest elastyczny i może być używany do przeglądania dowolnych zbiorów danych w formacie JSON z kluczami `id`, `text` i `summary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Sample from Train Dataset:\n",
      "ID: NPR-39426\n",
      "Text:\n",
      "Time now for business news. Thanks to a successful bid, the leading port operator in the Middle East will now control six terminals at U.S. ports. Dubai Ports World won a bidding contest for the United Kingdom's P&O ports. Some in the United States are expressing security concerns about the United Arab Emerits controlling American ports. NPR's Adam Davidson has the story. New York Senator Charles Schumer is taking the lead on attacking the deal by using language sure to get any New Yorker's attention. In our post-911 world, you can't be too careful. And the fact that the United Arab Emerits, even though it is now an ally of the United States, was the place where many of the hijackers came, should ring alarm bells when we hear that a company controlled by the United Arab Emerits is in charge of security now at our ports. Schumer painted a grim picture of potential terrorists taking over our nations entryways. This deal would give this company, Dubai Ports, control of our ports in New York, New Jersey, Baltimore, New Orleans, Miami, Norfolk, and Philadelphia. However scary Schumer's remarks are, his facts could be disputed. While two of the 911 hijackers are thought to have come from the United Arab Emerits, a loose alliance of Arab states, none are from Dubai, the country in question. And the company will not control all of any ports, says Steve Coleman, spokesman for the Port Authority in New York and New Jersey. No. Dubai Port World would have control over one terminal in the port of New York and New Jersey. In total, Dubai Ports World would have contracts to load and unload ships on a handful of terminals at six United States ports. But, Coleman says, lots of local and federal agencies will be there, too. U.S. Customs is in charge of cargo container security. Immigration officials check cargo to see if any people have been snuck on board. Port Authority police make sure the terminals stay secure. The Coast Guard would be responsible for anything that happens before the actual cargo container ships arrive here from out in the ocean, and through the various port channels. For all the perceived fear, Coleman says, the Dubai would find it very difficult to sneak any terrorists, weapons, or any other bad things past all those feds, if it wanted to. Stephen Flynn is a former Coast Guard commander, who now studies port security for the Council on Foreign Relations. There's a lot to be worried about. But this particular purchase of terminal operations inside the United States by Dubai Ports, I would not place at the top of my list of concerns. Flynn points out that Dubai is a close United States ally. Dubai Ports World manages terminal operations all over the globe, and there are far more real fears. What people should be most worried about is that when containers are stopped at the point of origin, when they're loaded up with cargo, it can be in some very scary neighborhoods. The container coming in to, say, New Jersey today, might have been loaded in an unsecure port in a dangerous part of the Middle East or Asia. It might have then traded hands a dozen or more times before making its way here. And the main security feature, the one thing that ensures the container holds what its shipper it holds, is a tiny plastic seal that costs a dollar, and is easily tampered with. Again, Stephen Flynn. We've got, essentially, a global network that moves millions of containers around the planet with very little in the way of adult supervision. Flynn says only a small percentage of containers are checked at all in the U.S., even though the technology exists to effectively x-ray every container that comes in to the country. But Flynn says he's not surprised that there's been more focus on the ownership of one part of certain ports, rather than on serious port security reform. What I find distressing is it remains, I think, so little understanding within the U.S. government, and within the American citizenry, about how a system we so depend upon in our society actually works. Flynn points out that pretty much everything we buy these days has at least some parts that came to the United States through a port. Ports have become utterly vital to our way of life. But, he says, if politicians and the public don't understand how they work, there's little hope that they can be properly secured. Adam Davidson, NPR News.\n",
      "\n",
      "Summary:\n",
      "A company based in the United Arab Emirates is taking over the operation of six American ports, including New York, Baltimore and New Orleans. Dubai Ports Worldwide is buying London-based Peninsular and Oriental Steam Navigation. New York Sen. Charles Schumer has criticized the deal, saying he is concerned about outsourcing services that affect national security.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Random Sample from Validation Dataset:\n",
      "ID: NPR-44861\n",
      "Text:\n",
      "After a historic vote, Ireland is moving forward today with steps to repeal its longtime ban on abortion. Ireland's health minister is meeting with the Irish Cabinet to begin to draft legislation for a new abortion law. This comes days after two-thirds of the country voted to repeal the eighth amendment to the constitution there, which had banned abortion. Joining us now is Fintan O'Toole. He's a columnist for The Irish Times. Thanks so much for being with us. FINTAN O'TOOLE: It's a pleasure, Rachel. After this vote, you wrote in The Times the following sentence - quote, \"this is the end of Irish exceptionalism. The Ireland of absolutes is dead and gone.\" Can you explain more what that means? O'TOOLE: Yeah. I suppose one of the reasons why Ireland had such extreme abortion laws - I mean, banning virtually all circumstances was - you know, the termination of pregnancy was banned - is that Ireland perhaps historically felt it had to compensate for its troubles, for the fact that it exported so many of its people, by holding itself up to the world as a source of exceptionally holy, Catholic place. And I think what we're seeing is that Ireland doesn't need to do that anymore, you know, that this was partly about abortion, but it was also partly about a society saying, we're willing to embrace our own reality. We're willing to embrace all of the women and men of Ireland, and we don't really need to feel that we're special anymore. Although, at the same time, you also wrote that there needs to be a consideration made for the people who voted to keep the eighth amendment or to keep the abortion ban, that they should not be made to feel like - this is your word - freaks in their own country. What's the risk of that? O'TOOLE: Yeah. Well, there is a risk, I think, you know, particularly because the vote was so overwhelming, as you said. It was two-thirds to one-third who voted to repeal the constitutional ban. So, of course, the people in that one-third are probably traditional people. Many of them would be people of faith. For a lot of them, this is a very, very dark moment. And I think it's important to say that Ireland shouldn't go down the road of the kind of culture wars that we've seen in the United States. You know, it's a small society. It's an intimate society. And actually, most Irish people probably agree about more things than they disagree about. And I just think it would be tragic if this was to become a kind of long-term battle between tradition and modernity. You know, there's no great strengths to be taken from that kind of battle for a society like Ireland. Well, then, what do you think the opposition will be? I mean, do you think all those people will just surrender the fight now? I mean, here in the United States, this is a battle that has gone to the state level, the local level, to try to change the abortion laws. Do you expect that these people who voted to keep the amendment will stop fighting? O'TOOLE: They won't. I mean, certainly a lot of them, you know, are very highly ideologically motivated, and they're very connected to, for example, the anti-abortion movements in the United States. So they will continue to make their point. However, one of the things that's really important here is that this was a popular vote, so they can't say this was the courts, this was an elite. This was the Irish people. And I think that gives this change an extraordinary mandate. You know, this really genuinely is what Irish people in the privacy of the polling stations when they went to mark their ballots, this is what they really felt. And you can't really deny that now. Fintan O'Toole, the columnist for The Irish Times, thanks so much for talking with us this morning. We appreciate it. O'TOOLE: A real pleasure.\n",
      "\n",
      "Summary:\n",
      "Two thirds of the nation voted to repeal the 8th amendment to its constitution, which banned abortion. Rachel Martin talks to Fintan O'Toole, a columnist for The Irish Times.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Random Sample from Test Dataset:\n",
      "ID: NPR-28753\n",
      "Text:\n",
      "Healthcare costs are important to consider when planning for retirement. This weekend, those costs will go up for military retirees. It's a modest increase but premiums for this group haven't gone up since the mid-'90s. The Defense Department announced this week the premium for retirees with a family plan will be $520 a year - up from $460. Active military members do not pay for healthcare. The military health program called TRICARE has been around for decades, and it may be under scrutiny as lawmakers work to cut spending. The cost of TRICARE in 2001 was $19 billion. Estimates say it now costs $53 billion.\n",
      "\n",
      "Summary:\n",
      "The Defense Department announced this week the premium for retirees with a family plan will be $520 a year, up from $460. Active military members do not pay for health care. The military health program, TRICARE, has been around for decades and may face scrutiny as lawmakers work to cut spending.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def display_random_sample(file_path, dataset_name):\n",
    "    \"\"\"\n",
    "    Displays a random sample from the given dataset file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the JSON file.\n",
    "        dataset_name (str): Name of the dataset (e.g., \"Train\", \"Validation\", \"Test\").\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            data = json.load(file)  # Load JSON data\n",
    "            if not data:\n",
    "                print(f\"The {dataset_name} dataset is empty!\")\n",
    "                return\n",
    "            \n",
    "            sample = random.choice(data)  # Select a random sample\n",
    "            print(f\"\\nRandom Sample from {dataset_name} Dataset:\")\n",
    "            print(f\"ID: {sample['id']}\")\n",
    "            print(f\"Text:\\n{sample['text']}\\n\")\n",
    "            print(f\"Summary:\\n{sample['summary']}\\n\")\n",
    "            print(\"-\" * 80)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {dataset_name} dataset: {e}\")\n",
    "\n",
    "# Display random samples from train, validation, and test datasets\n",
    "display_random_sample(\"./datasets/splits_filtered_with_summary/train.json\", \"Train\")\n",
    "display_random_sample(\"./datasets/splits_filtered_with_summary/val.json\", \"Validation\")\n",
    "display_random_sample(\"./datasets/splits_filtered_with_summary/test.json\", \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicjalizacja modelu BART, optymalizatora i gradient scaler\n",
    "\n",
    "Ten fragment kodu przygotowuje model BART do trenowania, definiuje optymalizator oraz wprowadza mechanizm mieszanej precyzji, co przyspiesza trening i redukuje użycie pamięci.\n",
    "\n",
    "### Szczegółowy opis:\n",
    "\n",
    "1. **Ładowanie modelu BART**:\n",
    "   - Model `BartForConditionalGeneration` wczytywany jest z pretrenowanego modelu `facebook/bart-base` za pomocą biblioteki Transformers.\n",
    "   - Model jest przenoszony na urządzenie określone w zmiennej `device` (MPS lub CPU).\n",
    "\n",
    "2. **Definiowanie optymalizatora**:\n",
    "   - Optymalizator `AdamW` jest inicjalizowany do aktualizacji wag modelu podczas procesu trenowania.\n",
    "   - Współczynnik uczenia (`lr`) jest ustawiany na wartość określoną w konfiguracji `CONFIG[\"learning_rate\"]`.\n",
    "\n",
    "3. **DataLoadery**:\n",
    "   - Funkcja `prepare_data_loaders` (zdefiniowana wcześniej) jest używana do stworzenia DataLoaderów dla zbiorów:\n",
    "     - Treningowego (`train_loader`).\n",
    "     - Walidacyjnego (`val_loader`).\n",
    "     - Testowego (`test_loader`).\n",
    "   - Rozmiar partii danych jest określony w konfiguracji `CONFIG[\"batch_size\"]`.\n",
    "\n",
    "4. **Inicjalizacja gradient scaler**:\n",
    "   - Mechanizm `GradScaler` z biblioteki PyTorch jest używany do trenowania w mieszanej precyzji.\n",
    "   - Pozwala to na wykonywanie obliczeń w precyzji FP16 tam, gdzie to możliwe, co przyspiesza trening i redukuje obciążenie pamięci.\n",
    "\n",
    "5. **Potwierdzenie inicjalizacji**:\n",
    "   - Po wczytaniu modelu, zdefiniowaniu optymalizatora i inicjalizacji gradient scaler, wyświetlany jest komunikat potwierdzający gotowość do treningu.\n",
    "\n",
    "### Wynik:\n",
    "- Model, optymalizator i gradient scaler są w pełni zainicjalizowane i gotowe do trenowania.\n",
    "- DataLoadery zapewniają efektywne ładowanie danych w partiach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, optimizer, and scaler initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/km/p88c5jp93pjd0_gb38mk5nkh0000gn/T/ipykernel_4677/92990305.py:21: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/Users/kamiljaworski/Projects/NLP_pro/venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Load the pre-trained BART model\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "\n",
    "# Define the DataLoaders (reuse from earlier)\n",
    "train_loader, val_loader, test_loader = prepare_data_loaders(\n",
    "    train_dataset, val_dataset, test_dataset, CONFIG[\"batch_size\"]\n",
    ")\n",
    "\n",
    "# Initialize gradient scaler for mixed precision\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Print model loaded\n",
    "print(\"Model, optimizer, and scaler initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcja ewaluacji modelu\n",
    "\n",
    "Ten fragment kodu definiuje funkcję `evaluate`, która ocenia model BART na podanym zbiorze danych, obliczając średnią wartość funkcji straty (loss).\n",
    "\n",
    "### Szczegółowy opis:\n",
    "\n",
    "1. **Definicja funkcji `evaluate`**:\n",
    "   - Argumenty:\n",
    "     - `model`: Model BART (`BartForConditionalGeneration`) do oceny.\n",
    "     - `data_loader`: DataLoader zawierający zbiór danych do ewaluacji.\n",
    "     - `device`: Urządzenie do obliczeń (`torch.device`), np. CPU lub GPU/MPS.\n",
    "   - Zwraca:\n",
    "     - Średnią wartość funkcji straty obliczoną dla całego zbioru danych.\n",
    "\n",
    "2. **Kroki w ewaluacji**:\n",
    "   - Model jest przełączany w tryb ewaluacji za pomocą `model.eval()`, co wyłącza warstwy specyficzne dla treningu, takie jak dropout.\n",
    "   - Gradienty są wyłączone (`torch.no_grad()`), aby zmniejszyć zużycie pamięci i przyspieszyć obliczenia.\n",
    "   - W każdej iteracji pętli:\n",
    "     - Dane wejściowe (`input_ids`) i maski uwagi (`attention_mask`) są przesyłane na urządzenie.\n",
    "     - Etykiety (`labels`) również są przesyłane na urządzenie.\n",
    "     - Model generuje wyjścia za pomocą funkcji `model`, a wartość funkcji straty (`loss`) jest akumulowana w zmiennej `running_loss`.\n",
    "\n",
    "3. **Obliczenie średniego loss**:\n",
    "   - Po zakończeniu przetwarzania całego zbioru danych, średnia strata jest obliczana przez podzielenie skumulowanej wartości przez liczbę batchy w DataLoaderze.\n",
    "\n",
    "4. **Wykorzystanie paska postępu**:\n",
    "   - Pętla przetwarzająca dane korzysta z `tqdm` do wyświetlenia paska postępu, co pozwala na monitorowanie postępu ewaluacji, który jest widoczny w kolejnym segmencie kodu.\n",
    "\n",
    "### Wynik:\n",
    "- Funkcja zwraca średnią wartość funkcji straty dla zbioru danych, co jest miarą jakości predykcji modelu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a given dataset.\n",
    "\n",
    "    Args:\n",
    "        model (BartForConditionalGeneration): The model to evaluate.\n",
    "        data_loader (DataLoader): DataLoader for the evaluation dataset.\n",
    "        device (torch.device): Device to use for computation.\n",
    "\n",
    "    Returns:\n",
    "        float: The average loss over the dataset.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            running_loss += outputs.loss.item()\n",
    "\n",
    "    return running_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning modelu BART na danych użytkownika\n",
    "\n",
    "Ten fragment kodu implementuje pętlę fine-tuningu dla wstępnie wytrenowanego modelu BART (`facebook/bart-base`) na danych użytkownika. Wykorzystywany jest mechanizm akumulacji gradientów w celu symulacji większego rozmiaru batcha, a także monitorowane są straty na zbiorach treningowym i walidacyjnym.\n",
    "\n",
    "### Szczegółowy opis:\n",
    "\n",
    "1. **Fine-tuning modelu BART**:\n",
    "   - Model `BartForConditionalGeneration` jest wstępnie wytrenowany na dużym korpusie danych (np. CNN/DailyMail), a poniższy kod doucza model na własnym zbiorze danych użytkownika.\n",
    "   - Model nie jest trenowany od zera, co pozwala zaoszczędzić czas i zasoby obliczeniowe, jednocześnie osiągając lepsze wyniki na nowych danych.\n",
    "\n",
    "2. **Inicjalizacja**:\n",
    "   - Listy `train_losses` i `val_losses` przechowują wartości strat (loss) z każdej epoki dla zbiorów treningowego i walidacyjnego.\n",
    "   - `gradient_accumulation_steps`: Liczba kroków akumulacji gradientów przed aktualizacją wag modelu, co pozwala na efektywne trenowanie przy ograniczonych zasobach.\n",
    "\n",
    "3. **Pętla fine-tuningu**:\n",
    "   - Pętla główna iteruje przez liczbę epok (`CONFIG[\"epochs\"]`).\n",
    "   - Model jest przełączany w tryb treningowy za pomocą `model.train()`.\n",
    "\n",
    "4. **Przetwarzanie batchy**:\n",
    "   - Dla każdej partii danych:\n",
    "     - Dane wejściowe (`input_ids`, `attention_mask`) i etykiety (`labels`) są przesyłane na urządzenie (`device`).\n",
    "     - Obliczana jest strata (`loss`) skalowana przez liczbę kroków akumulacji gradientów.\n",
    "     - Gradienty są propagowane wstecz (`loss.backward()`).\n",
    "     - Gradienty są przycinane (`clip_grad_norm_`) w celu zapobiegania ich eksplozji.\n",
    "\n",
    "5. **Aktualizacja wag**:\n",
    "   - Po określonej liczbie kroków akumulacji gradientów (`gradient_accumulation_steps`), wagi są aktualizowane za pomocą `optimizer.step()`, a gradienty zerowane (`optimizer.zero_grad()`).\n",
    "\n",
    "6. **Walidacja**:\n",
    "   - Po zakończeniu przetwarzania batchy w epokach, model jest ewaluowany na zbiorze walidacyjnym przy użyciu funkcji `evaluate`.\n",
    "   - Średnia strata na zbiorze walidacyjnym jest dodawana do listy `val_losses`.\n",
    "\n",
    "7. **Wykres strat**:\n",
    "   - Po zakończeniu fine-tuningu generowany jest wykres porównujący straty treningowe i walidacyjne w każdej epoce.\n",
    "\n",
    "### Wynik:\n",
    "- Model BART jest fine-tunowany na danych użytkownika przez określoną liczbę epok.\n",
    "- Generowany wykres umożliwia wizualizację procesu trenowania i monitorowanie konwergencji modelu.\n",
    "\n",
    "### Uwagi:\n",
    "- **Fine-tuning**: Dzięki użyciu wstępnie wytrenowanego modelu proces wymaga znacznie mniej danych niż trenowanie od zera.\n",
    "- **Akumulacja gradientów**: Umożliwia użycie większego efektywnego rozmiaru batcha, co jest szczególnie przydatne przy ograniczonej pamięci GPU.\n",
    "- **Monitorowanie strat**: Pozwala ocenić, czy model nie przeucza się, obserwując różnice między stratą treningową a walidacyjną.\n",
    "\n",
    "### Wymagania:\n",
    "- Model i optymalizator muszą być wcześniej zainicjalizowane z użyciem pretrenowanego modelu BART.\n",
    "- Wymagana jest funkcja `evaluate`, aby obliczyć stratę na zbiorze walidacyjnym.\n",
    "- Do wizualizacji strat wymagany jest pakiet Matplotlib (`pip install matplotlib`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2470/2470 [36:02<00:00,  1.14it/s]\n",
      "Evaluating: 100%|██████████| 1235/1235 [05:30<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.9328, Validation Loss: 0.2212\n",
      "\n",
      "Epoch 2/5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2470/2470 [37:16<00:00,  1.10it/s]\n",
      "Evaluating: 100%|██████████| 1235/1235 [05:35<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2296, Validation Loss: 0.2185\n",
      "\n",
      "Epoch 3/5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2470/2470 [36:40<00:00,  1.12it/s]\n",
      "Evaluating: 100%|██████████| 1235/1235 [05:32<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2142, Validation Loss: 0.2176\n",
      "\n",
      "Epoch 4/5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2470/2470 [36:48<00:00,  1.12it/s]\n",
      "Evaluating: 100%|██████████| 1235/1235 [05:29<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2020, Validation Loss: 0.2194\n",
      "\n",
      "Epoch 5/5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2470/2470 [36:44<00:00,  1.12it/s]\n",
      "Evaluating: 100%|██████████| 1235/1235 [05:33<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1911, Validation Loss: 0.2199\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training loop\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Initialize lists for tracking training and validation losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Gradient accumulation steps to simulate a larger batch size\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "for epoch in range(CONFIG[\"epochs\"]):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{CONFIG['epochs']}:\")\n",
    "\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "\n",
    "    # Iterate over batches in the training DataLoader\n",
    "    for step, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass (without mixed precision)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss / gradient_accumulation_steps  # Scale loss for gradient accumulation\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Update weights after every `gradient_accumulation_steps`\n",
    "        if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(train_loader):\n",
    "            optimizer.step()  # Apply optimizer step\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "        # Track running loss\n",
    "        running_loss += loss.item() * gradient_accumulation_steps\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Evaluate the model on the validation dataset\n",
    "    val_loss = evaluate(model, val_loader, device)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Print epoch results\n",
    "    print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, CONFIG[\"epochs\"] + 1), train_losses, label=\"Training Loss\")\n",
    "plt.plot(range(1, CONFIG[\"epochs\"] + 1), val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wnioski z fine-tuningu modelu BART\n",
    "\n",
    "Proces fine-tuningu modelu BART na danych użytkownika wykazał poprawną konwergencję modelu w początkowych epokach, z wyraźnym spadkiem straty treningowej i walidacyjnej. Jednak w późniejszych epokach zaobserwowano symptomy przeuczenia (overfitting), gdy strata walidacyjna przestała spadać, mimo dalszego zmniejszania się straty treningowej.\n",
    "\n",
    "### Kluczowe obserwacje:\n",
    "1. **Konwergencja modelu**:\n",
    "   - Model szybko nauczył się reprezentacji danych użytkownika w początkowych epokach, co widoczne było w gwałtownym spadku straty treningowej i walidacyjnej.\n",
    "\n",
    "2. **Przeuczenie**:\n",
    "   - W późniejszych epokach strata walidacyjna ustabilizowała się, a następnie nieznacznie wzrosła, co wskazuje na tendencję modelu do nadmiernego dopasowania się do danych treningowych.\n",
    "\n",
    "3. **Dobry moment zatrzymania treningu**:\n",
    "   - Trening został zakończony w odpowiednim momencie, gdy dalsze epoki nie przynosiłyby już istotnej poprawy na zbiorze walidacyjnym.\n",
    "\n",
    "---\n",
    "\n",
    "### Zalecenia:\n",
    "- **Eksperymenty z hiperparametrami**:\n",
    "  - Warto przeprowadzić eksperymenty z różnymi wartościami hiperparametrów, takich jak:\n",
    "    - Rozmiar batcha (`batch_size`).\n",
    "    - Liczba tokenów (`max_length`).\n",
    "    - Współczynnik uczenia (`learning_rate`).\n",
    "    - Regularizacja, np. przez mechanizm dropout lub `weight_decay`.\n",
    "\n",
    "- **Zróżnicowanie danych**:\n",
    "  - Można poprawić jakość treningu poprzez zwiększenie różnorodności danych wejściowych, co pomoże modelowi lepiej generalizować.\n",
    "\n",
    "---\n",
    "\n",
    "### Monitorowanie jakości generacji:\n",
    "- Szczegółowa analiza generowanych streszczeń za pomocą metryk takich jak BLEU czy ROUGE będzie przeprowadzona w osobnym notatniku. To pozwoli na dokładniejszą ocenę jakości modelu w praktycznych zastosowaniach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zapis modelu BART i tokenizer-a\n",
    "\n",
    "Ten fragment kodu zapisuje wytrenowany model BART oraz odpowiadający mu tokenizer do określonego katalogu, co pozwala na ich późniejsze wykorzystanie w zadaniach takich jak generacja tekstu czy ewaluacja.\n",
    "\n",
    "### Szczegółowy opis:\n",
    "\n",
    "1. **Funkcja `save_model`**:\n",
    "   - Zapisuje model i tokenizer do określonego katalogu.\n",
    "   - Argumenty:\n",
    "     - `model`: Wytrenowany model BART (`BartForConditionalGeneration`).\n",
    "     - `tokenizer`: Tokenizer odpowiadający modelowi.\n",
    "     - `model_name`: Nazwa katalogu, w którym zapisany zostanie model i tokenizer (domyślnie `model_v3`).\n",
    "\n",
    "2. **Tworzenie katalogu**:\n",
    "   - Ścieżka zapisu jest tworzona w katalogu `./models` z podaną nazwą modelu (`model_name`).\n",
    "   - Jeśli katalog już istnieje, jest wykorzystywany bez tworzenia nowego.\n",
    "\n",
    "3. **Zapisywanie modelu i tokenizer-a**:\n",
    "   - `model.save_pretrained(save_dir)`: Zapisuje model w formacie zgodnym z biblioteką Transformers.\n",
    "   - `tokenizer.save_pretrained(save_dir)`: Zapisuje tokenizer w tym samym katalogu.\n",
    "\n",
    "4. **Komunikaty użytkowe**:\n",
    "   - Informacje o lokalizacji zapisu są drukowane w konsoli, aby potwierdzić poprawne działanie.\n",
    "\n",
    "5. **Wywołanie funkcji**:\n",
    "   - Funkcja `save_model` jest wywoływana dla aktualnie wytrenowanego modelu i tokenizer-a, z domyślną nazwą `model_v3`.\n",
    "\n",
    "### Wynik:\n",
    "- Model i tokenizer są zapisywane w katalogu `./models/model_v3`.\n",
    "- Pliki modelu (`pytorch_model.bin`, `config.json`) oraz tokenizer-a (`vocab.json`, `merges.txt`, `tokenizer_config.json`) są przechowywane w tym samym katalogu.\n",
    "\n",
    "### Uwagi:\n",
    "- Katalog `./models` jest domyślnym miejscem przechowywania modeli. Można go zmienić, modyfikując ścieżkę w kodzie.\n",
    "- Model i tokenizer mogą być później wczytane za pomocą funkcji `BartForConditionalGeneration.from_pretrained` i `BartTokenizer.from_pretrained`.\n",
    "- Zapewnienie unikalnej nazwy dla każdego modelu (`model_name`) pozwala na przechowywanie wielu wersji w jednym katalogu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zapisywanie modelu w: ./models/model_v3\n",
      "Model i tokenizer zapisane w katalogu: ./models/model_v3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "model_name = \"model_v3\"\n",
    "\n",
    "def save_model(model, tokenizer, model_name=model_name):\n",
    "    \"\"\"\n",
    "    Save the model and tokenizer to a specific directory.\n",
    "\n",
    "    Args:\n",
    "        model (BartForConditionalGeneration): The trained model.\n",
    "        tokenizer (BartTokenizer): Tokenizer associated with the model.\n",
    "        model_name (str): Name of the model directory (e.g., \"model1\").\n",
    "    \"\"\"\n",
    "    # Tworzenie ścieżki katalogu\n",
    "    save_dir = os.path.join(\"./models\", model_name)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Zapisywanie modelu i tokenizer-a\n",
    "    print(f\"Zapisywanie modelu w: {save_dir}\")\n",
    "    model.save_pretrained(save_dir)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    print(f\"Model i tokenizer zapisane w katalogu: {save_dir}\")\n",
    "\n",
    "# Wywołanie funkcji zapisu\n",
    "save_model(model, tokenizer, model_name = model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wczytywanie zapisanego modelu BART i tokenizer'a\n",
    "\n",
    "Ten fragment kodu umożliwia wczytanie wcześniej zapisanego modelu BART oraz tokenizer-a z podanego katalogu.\n",
    "\n",
    "### Szczegółowy opis:\n",
    "\n",
    "1. **Funkcja `load_model`**:\n",
    "   - Argumenty:\n",
    "     - `model_name`: Nazwa katalogu, w którym zapisano model (domyślnie `model_v3`).\n",
    "   - Zwraca:\n",
    "     - `model`: Wczytany model BART (`BartForConditionalGeneration`).\n",
    "     - `tokenizer`: Wczytany tokenizer.\n",
    "\n",
    "2. **Proces wczytywania**:\n",
    "   - Model i tokenizer są wczytywane z katalogu `./models/{model_name}` za pomocą metod `from_pretrained`.\n",
    "\n",
    "3. **Przykład użycia**:\n",
    "   - Wczytywanie modelu z katalogu `model_v3`:\n",
    "     ```python\n",
    "     loaded_model, loaded_tokenizer = load_model(model_name=\"model_v3\")\n",
    "     ```\n",
    "\n",
    "### Wynik:\n",
    "- Model i tokenizer są gotowe do użycia w aplikacjach NLP, takich jak generacja tekstu czy ewaluacja.\n",
    "\n",
    "### Uwagi:\n",
    "- Katalog `./models/{model_name}` musi zawierać wcześniej zapisany model i tokenizer.\n",
    "- Funkcja jest zgodna z formatem zapisu w bibliotece Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wczytywanie modelu z katalogu: ./models/model_v3\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "model_name = \"model_v3\"\n",
    "\n",
    "def load_model(model_name=model_name):\n",
    "    \"\"\"\n",
    "    Load a saved model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the saved model directory (e.g., \"model1\").\n",
    "\n",
    "    Returns:\n",
    "        tuple: Loaded model and tokenizer.\n",
    "    \"\"\"\n",
    "    # Ścieżka do zapisanego modelu\n",
    "    load_dir = os.path.join(\"./models\", model_name)\n",
    "\n",
    "    # Wczytanie modelu i tokenizer-a\n",
    "    print(f\"Wczytywanie modelu z katalogu: {load_dir}\")\n",
    "    model = BartForConditionalGeneration.from_pretrained(load_dir)\n",
    "    tokenizer = BartTokenizer.from_pretrained(load_dir)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Przykład wczytania modelu\n",
    "loaded_model, loaded_tokenizer = load_model(model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generowanie podsumowań z modelu BART i porównanie wyników\n",
    "\n",
    "Ten fragment kodu wczytuje wcześniej zapisany model BART, generuje podsumowanie dla losowo wybranego artykułu z zestawu testowego, i porównuje je z podsumowaniem zawartym w danych.\n",
    "\n",
    "### Szczegółowy opis:\n",
    "\n",
    "1. **Wczytanie modelu i tokenizer-a**:\n",
    "   - Funkcja `load_model` wczytuje model i tokenizer z katalogu `./models/{model_name}`.\n",
    "   - Model jest przenoszony na urządzenie (`device`) przed generowaniem podsumowań.\n",
    "\n",
    "2. **Wczytanie danych testowych**:\n",
    "   - Funkcja `load_test_data` ładuje dane testowe z pliku JSON podanego w zmiennej `test_file_path`.\n",
    "\n",
    "3. **Generowanie podsumowania**:\n",
    "   - Funkcja `generate_summary` wykorzystuje model BART do wygenerowania podsumowania dla zadanego tekstu.\n",
    "   - Parametry podsumowania:\n",
    "     - `max_length`: Maksymalna długość wygenerowanego podsumowania (domyślnie 150 tokenów).\n",
    "     - `min_length`: Minimalna długość wygenerowanego podsumowania (domyślnie 40 tokenów).\n",
    "     - `num_beams`: Liczba ścieżek eksplorowanych podczas generacji (domyślnie 4).\n",
    "\n",
    "4. **Porównanie wyników**:\n",
    "   - Losowy artykuł z danych testowych (`random_article`) jest wybrany do analizy.\n",
    "   - Wyświetlane są:\n",
    "     - Oryginalny tekst artykułu.\n",
    "     - Podsumowanie zapisane w zestawie danych (`Dataset Summary`).\n",
    "     - Podsumowanie wygenerowane przez model (`Model-Generated Summary`).\n",
    "\n",
    "5. **Wyświetlanie wyników**:\n",
    "   - Tekst i podsumowania są formatowane za pomocą biblioteki `textwrap` dla lepszej czytelności (maksymalna szerokość linii: 80 znaków).\n",
    "\n",
    "### Wynik:\n",
    "- Kod generuje podsumowanie dla losowego artykułu i wyświetla porównanie pomiędzy:\n",
    "  - Oryginalnym tekstem.\n",
    "  - Podsumowaniem zapisanym w zestawie danych.\n",
    "  - Podsumowaniem wygenerowanym przez model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Porównanie ===\n",
      "\n",
      "Oryginalny tekst:\n",
      "This is MORNING EDITION from NPR News. Good morning. I'm Renee Montagne. NPR's\n",
      "Scott Horsley was along for the ride and has this report. President Obama hosted\n",
      "four town hall meetings and an economic conference during this bus tour. But the\n",
      "heart of the trip came in-between those big scheduled events, as Mr. Obama\n",
      "traveled the countryside, visiting with high school sports teams, stopping at a\n",
      "county fair, and saluting uniformed firefighters standing at attention outside\n",
      "their small town firehouse. You're passing rows of kids with flags and\n",
      "grandparents in their lawn chairs and mechanics out in front of their shops and,\n",
      "you know, farmers waving from their fields, and it inspires you. 'Cause it\n",
      "reminds you about what makes this country so great. Some of the people lining\n",
      "the street to shake hands with Mr. Obama remember his first presidential\n",
      "campaign four years ago, back when his hair was not so grey. Yeah, somebody had\n",
      "like an old flyer I'd send him. I looked so young. Years of recession have taken\n",
      "their toll on the president and on millions of other Americans. Mr. Obama spent\n",
      "a lot of time this week talking about the need to get the economy growing\n",
      "faster. On Tuesday, he had breakfast with a group of small business owners like\n",
      "Michael Sexton. Sexton runs a software company called Manure Works that helps\n",
      "farmers keep track their fertilizer applications. Sexton is a former Republican\n",
      "lawmaker in Iowa, but says he's willing to take off his partisan hat for the\n",
      "sake of economic development. At breakfast, he found himself nodding in\n",
      "agreement with his fellow business owners. Basically all of us were on the same\n",
      "page as each one of us spoke to the president about helping rural Iowa and\n",
      "revitalizing small communities and helping small business. Mr. Obama sees plenty\n",
      "of that kind of bipartisan cooperation outside Washington, even if there's\n",
      "precious little in the nation's capital. At a forum on rural development in\n",
      "Peosta, Iowa this week, he said it was hard to tell which participants were\n",
      "Democrats or Republicans. And so it's a very practical way of thinking about\n",
      "these problems. It's not either/or. It's a recognition that the prime driver of\n",
      "economic growth and jobs is going to be our people and the private sector and\n",
      "our businesses. But you know what? Government can help. Government can make a\n",
      "difference. That's the brand of conciliatory politics that Mr. Obama ran on back\n",
      "in 2008. But the president also showed a more combative side on this trip,\n",
      "trying to enlist voters in what he called a battle with lawmakers who aren't\n",
      "doing enough to encourage job growth. To be sure, the president offered few new\n",
      "ideas for generating jobs this week, instead recycling proposals to extend the\n",
      "payroll tax cut and increase federal financing for public works projects. Mr.\n",
      "Obama says in the weeks to come, he will propose new jobs measures, along with\n",
      "steps to reduce the deficit, and he promises to call out Republican lawmakers\n",
      "who fail to act on them. I can't force them to do it. You can force them to do\n",
      "it. And I will take my case to the American people that this vision is how we\n",
      "move the country forward. And if they've got an alternative vision and they\n",
      "don't want to sit there and do nothing for the next 16 months while unemployment\n",
      "is still high and small businesses are still suffering, then ultimately they're\n",
      "going to be held to account by you, just like I'm going to be held to account by\n",
      "you. But that accounting is still more than a year away. And in these waning\n",
      "days of summer, Mr. Obama appears to be biding his time. Not so long ago, before\n",
      "the grand bargain with Republicans collapsed, the president was challenging\n",
      "Americans to eat our peas. Now he's settling for soft-serve instead.\n",
      "Unidentified Woman: It is. All right... Mr. Obama treated his traveling staff to\n",
      "half-a-dozen ice cream cones at Dewitt Dairy Treats in Dewitt, Iowa. Owner Kris\n",
      "Lyon just smiled while snapping pictures with her cell phone. If all these\n",
      "people that came to see him come and get ice cream, then we'll be doing okay,\n",
      "huh? Scott Horsley, NPR News, Alpha, Illinois.\n",
      "\n",
      "Streszczenie z datasetu:\n",
      "President Obama is off to New England on Thursday afternoon for a family\n",
      "vacation. He returned to the White House Wednesday night after a three-day bus\n",
      "tour through the upper Midwest. The campaign-style trip produced little in the\n",
      "way of new ideas for boosting job growth.\n",
      "\n",
      "Wygenerowane streszczenie przez model:\n",
      "President Obama spent a lot of time this week talking about the need to get the\n",
      "economy growing faster. At a forum on rural development in Peosta, Iowa, he said\n",
      "it was hard to tell which participants were Democrats or Republicans.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import textwrap\n",
    "\n",
    "\n",
    "def load_model(model_name, base_dir=\"./models\"):\n",
    "    \"\"\"\n",
    "    Load a saved model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the saved model directory.\n",
    "        base_dir (str): Base directory where models are saved.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Loaded model and tokenizer.\n",
    "    \"\"\"\n",
    "    model_path = os.path.join(base_dir, model_name)\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def load_test_data(file_path):\n",
    "    \"\"\"\n",
    "    Load test data from a JSON file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        list: List of articles from the dataset.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "\n",
    "def generate_summary(model, tokenizer, text, max_length=150, min_length=40):\n",
    "    \"\"\"\n",
    "    Generate a summary for the given text.\n",
    "\n",
    "    Args:\n",
    "        model: Pre-trained BART model.\n",
    "        tokenizer: BART tokenizer.\n",
    "        text (str): Input text to summarize.\n",
    "        max_length (int): Maximum length of the summary.\n",
    "        min_length (int): Minimum length of the summary.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated summary.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "        length_penalty=2.0,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"model_v3\"  # Specify the model name\n",
    "model, tokenizer = load_model(model_name)\n",
    "model.to(device)\n",
    "\n",
    "# Load test dataset\n",
    "test_file_path = \"./datasets/splits_filtered_with_summary/test.json\"\n",
    "test_data = load_test_data(test_file_path)\n",
    "\n",
    "# Select a random article\n",
    "random_article = random.choice(test_data)\n",
    "\n",
    "# Extract original text and summaries\n",
    "original_text = random_article[\"text\"]\n",
    "dataset_summary = random_article[\"summary\"]\n",
    "generated_summary = generate_summary(model, tokenizer, original_text)\n",
    "\n",
    "# Display comparison side-by-side\n",
    "print(\"=== Comparison ===\\n\")\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(textwrap.fill(original_text, width=80))\n",
    "\n",
    "print(\"\\nDataset Summary:\")\n",
    "print(textwrap.fill(dataset_summary, width=80))\n",
    "\n",
    "print(\"\\nModel-Generated Summary:\")\n",
    "print(textwrap.fill(generated_summary, width=80))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
