{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie datasetu\n",
    "\n",
    "Ten segment kodu służy do przygotowania datasetu. Wczytuje pliki JSON zawierające informacje o podziale na zbiory treningowe, walidacyjne i testowe oraz duży plik z danymi dialogowymi. Kod jest zoptymalizowany do pracy z dużymi plikami JSON, zapewniając efektywne zarządzanie pamięcią.\n",
    "\n",
    "### Szczegółowy opis:\n",
    "\n",
    "1. **Definicja ścieżek do plików**:\n",
    "   - `SPLIT_FILE_PATH`: Ścieżka do pliku zawierającego podział na zbiory treningowe, walidacyjne i testowe.\n",
    "   - `NEWSDIALOG_FILE_PATH`: Ścieżka do dużego pliku JSON z dialogami.\n",
    "\n",
    "2. **Wczytanie danych podziału**:\n",
    "   - Dane z pliku `train_val_test_split.json` są wczytywane do zmiennej `split_data` przy użyciu biblioteki `json`.\n",
    "\n",
    "3. **Funkcja do wczytywania dużego pliku JSON**:\n",
    "   - `load_large_json(file_path)` to funkcja generatora, która wczytuje plik JSON linia po linii, umożliwiając przetwarzanie dużych plików bez obciążania pamięci.\n",
    "\n",
    "4. **Potwierdzenie załadowania plików**:\n",
    "   - Po wczytaniu danych wyświetla komunikat: „Files have been successfully loaded.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files have been successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "# File paths\n",
    "SPLIT_FILE_PATH = \"/Users/kamiljaworski/Projects/NLP_pro/datasets/MediaSum/data/train_val_test_split.json\"\n",
    "NEWSDIALOG_FILE_PATH = \"/Users/kamiljaworski/Projects/NLP_pro/datasets/MediaSum/data/news_dialogue.json\"\n",
    "\n",
    "# Load split data\n",
    "with open(SPLIT_FILE_PATH, \"r\") as split_file:\n",
    "    split_data = json.load(split_file)\n",
    "\n",
    "# Function to load a large JSON file line by line\n",
    "def load_large_json(file_path):\n",
    "    \"\"\"\n",
    "    Generator function to load a large JSON file line by line.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the JSON file.\n",
    "        \n",
    "    Yields:\n",
    "        dict: Parsed JSON object for each line in the file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            yield json.loads(line)\n",
    "\n",
    "print(\"Files have been successfully loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pobieranie próbek z danych\n",
    "\n",
    "Ten fragment kodu służy do pobierania określonej liczby próbek z dostępnych danych podziału na zbiory `train`, `val` i `test`. Przydatny jest w przypadkach, gdy potrzebujesz losowego podzbioru danych do eksperymentów lub testów.\n",
    "\n",
    "### Szczegółowy opis:\n",
    "\n",
    "1. **Funkcja `sample_train_ids`**:\n",
    "   - Pobiera losową próbkę ID z połączonych zbiorów `train`, `val` i `test`.\n",
    "   - Argumenty:\n",
    "     - `split_data`: Słownik zawierający dane podziału.\n",
    "     - `sample_size`: Liczba próbek do wybrania.\n",
    "   - Funkcja drukuje liczbę unikalnych ID oraz podnosi błąd, jeśli żądana próba przekracza dostępne dane.\n",
    "\n",
    "2. **Łączenie danych**:\n",
    "   - Wszystkie ID ze zbiorów `train`, `val` i `test` są łączone w jedną listę.\n",
    "\n",
    "3. **Próba losowa**:\n",
    "   - Funkcja `random.sample` wybiera określoną liczbę ID w sposób losowy, bez powtórzeń.\n",
    "\n",
    "4. **Zmienne globalne**:\n",
    "   - `accumulated_ids`: Liczba wszystkich ID w danych.\n",
    "   - `SAMPLE_SIZE`: Określa wielkość próby (tutaj równa liczbie wszystkich ID).\n",
    "\n",
    "5. **Wynik**:\n",
    "   - Wyświetlana jest liczba wybranych próbek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique IDs: 463596\n",
      "Selected 463596 samples.\n"
     ]
    }
   ],
   "source": [
    "def sample_train_ids(split_data, sample_size):\n",
    "    \"\"\"\n",
    "    Selects a specified number of samples from the `train`, `val`, and `test` datasets.\n",
    "\n",
    "    Args:\n",
    "        split_data (dict): Dictionary containing split data.\n",
    "        sample_size (int): Number of samples to select.\n",
    "\n",
    "    Returns:\n",
    "        list: List of selected IDs.\n",
    "    \"\"\"\n",
    "    # Combine IDs from train, validation, and test splits\n",
    "    train_ids = split_data[\"train\"] + split_data[\"val\"] + split_data[\"test\"]\n",
    "    \n",
    "    # Print the number of unique IDs\n",
    "    print(f\"Number of unique IDs: {len(train_ids)}\")\n",
    "    \n",
    "    # Raise an error if the sample size exceeds the total number of IDs\n",
    "    if sample_size > len(train_ids):\n",
    "        raise ValueError(\n",
    "            f\"Sample size ({sample_size}) exceeds the number of available data points ({len(train_ids)}).\"\n",
    "        )\n",
    "    \n",
    "    # Randomly sample the specified number of IDs\n",
    "    return random.sample(train_ids, sample_size)\n",
    "\n",
    "# Calculate the total number of IDs across all splits\n",
    "accumulated_ids = len(split_data[\"train\"] + split_data[\"val\"] + split_data[\"test\"])\n",
    "\n",
    "# Define the sample size as the total number of IDs\n",
    "SAMPLE_SIZE = accumulated_ids\n",
    "\n",
    "# Select a sample of IDs\n",
    "sampled_ids = sample_train_ids(split_data, SAMPLE_SIZE)\n",
    "\n",
    "# Print the number of selected samples\n",
    "print(f\"Selected {len(sampled_ids)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wyodrębnianie danych na podstawie ID\n",
    "\n",
    "Ten fragment kodu wyodrębnia artykuły i ich streszczenia na podstawie podanych ID z dużego pliku JSON. Wyodrębnione dane są następnie zapisywane do nowego pliku JSON.\n",
    "\n",
    "### Szczegółowy opis:\n",
    "\n",
    "1. **Funkcja `extract_data_by_ids`**:\n",
    "   - Wyodrębnia artykuły zawierające dane `id`, `text`, oraz `summary` z pliku JSON.\n",
    "   - Argumenty:\n",
    "     - `news_file_path`: Ścieżka do pliku `NewsDialog.json`.\n",
    "     - `ids_to_extract`: Lista ID, które mają zostać wyodrębnione.\n",
    "   - Zwraca listę słowników z polami:\n",
    "     - `id`: ID artykułu.\n",
    "     - `text`: Połączony tekst artykułu.\n",
    "     - `summary`: Streszczenie artykułu.\n",
    "\n",
    "2. **Szybkie wyszukiwanie ID**:\n",
    "   - Wykorzystano `set` dla szybszego sprawdzania obecności ID.\n",
    "\n",
    "3. **Warunki zakończenia**:\n",
    "   - Pętla zatrzymuje się po przetworzeniu wszystkich ID z listy, co przyspiesza działanie kodu.\n",
    "\n",
    "4. **Zapis danych**:\n",
    "   - Wyodrębnione dane są zapisywane do pliku `extracted_samples.json` w formacie JSON z wcięciami dla czytelności.\n",
    "\n",
    "### Wynik:\n",
    "- Kod wyświetla liczbę wyodrębnionych artykułów oraz ścieżkę do zapisanego pliku JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 463596 articles.\n",
      "Extracted data saved to file: ./datasets/extracted_samples.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def extract_data_by_ids(news_file_path, ids_to_extract):\n",
    "    \"\"\"\n",
    "    Extracts articles and summaries based on provided IDs.\n",
    "\n",
    "    Args:\n",
    "        news_file_path (str): Path to the NewsDialog.json file.\n",
    "        ids_to_extract (list): List of IDs to extract.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of articles containing ID, text, and summary.\n",
    "    \"\"\"\n",
    "    extracted_data = []\n",
    "    ids_set = set(ids_to_extract)  # Use a set for faster lookups\n",
    "\n",
    "    # Open and load the JSON file\n",
    "    with open(news_file_path, \"r\") as file:\n",
    "        news_data = json.load(file)  # Load the entire JSON file (list)\n",
    "\n",
    "        for article in news_data:\n",
    "            # Check if \"id\" exists and matches one of the selected IDs\n",
    "            if \"id\" in article and article[\"id\"] in ids_set:\n",
    "                extracted_data.append({\n",
    "                    \"id\": article[\"id\"],\n",
    "                    \"text\": \" \".join(article.get(\"utt\", [])),  # Join sentences\n",
    "                    \"summary\": article.get(\"summary\", \"\")  # Get the summary\n",
    "                })\n",
    "            # Stop when all IDs have been processed\n",
    "            if len(extracted_data) == len(ids_to_extract):\n",
    "                break  \n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "# Extract data\n",
    "extracted_samples = extract_data_by_ids(NEWSDIALOG_FILE_PATH, sampled_ids)\n",
    "print(f\"Extracted {len(extracted_samples)} articles.\")\n",
    "\n",
    "# Save extracted data to a JSON file\n",
    "output_file = \"./datasets/extracted_samples.json\"\n",
    "with open(output_file, \"w\") as file:\n",
    "    json.dump(extracted_samples, file, indent=4)\n",
    "print(f\"Extracted data saved to file: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wyświetlanie losowych przykładów\n",
    "\n",
    "Ten fragment kodu losowo wybiera dwa przykłady z wyodrębnionych artykułów i wyświetla ich zawartość, w tym ID, tekst oraz streszczenie.\n",
    "\n",
    "### Szczegółowy opis:\n",
    "\n",
    "1. **Losowy wybór przykładów**:\n",
    "   - Używana jest funkcja `random.sample`, aby wybrać dwa losowe przykłady z listy `extracted_samples`.\n",
    "\n",
    "2. **Wyświetlanie wyników**:\n",
    "   - Dla każdego przykładu drukowane są:\n",
    "     - `Article ID`: Unikalny identyfikator artykułu.\n",
    "     - `Text`: Tekst artykułu (łączony z poszczególnych zdań).\n",
    "     - `Summary`: Streszczenie artykułu.\n",
    "   - Przykłady są oddzielone linią składającą się ze 100 znaków `-`, aby poprawić czytelność.\n",
    "\n",
    "### Wynik:\n",
    "- Po uruchomieniu kodu zobaczysz dwa losowe artykuły wybrane z wyodrębnionych danych, z pełnym tekstem i streszczeniem.\n",
    "\n",
    "### Uwagi:\n",
    "- Kod jest prosty i efektywny, pozwalając szybko zweryfikować zawartość wyodrębnionych danych.\n",
    "- Może być łatwo zmodyfikowany, aby wybrać inną liczbę przykładów, zmieniając wartość w funkcji `random.sample`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article ID: CNN-407734\n",
      "Text:\n",
      "As the total number of coronavirus cases in the U.S. nears 5 million, we're now learning just how quickly the virus is spreading inside the U.S. federal prison system. CNN's Drew Griffin has our report. The Seagoville Federal Correctional Institution outside Dallas is a petri dish of coronavirus infection. I lost my smell. I lived in the restroom for like 12 days. In the 15-minute phone call allowed from the inside, inmate George Reagan explained how coronavirus swept through the facility in just a month. Everybody has it now. Seagoville has significantly more COVID-19 cases than any other federal prison in the U.S. More than 1,300 of the roughly 1,750 inmates there tested positive. That's 75 percent. George Reagan's wife, Tabitha, that says no visitors have been allowed at the prison for months, so she believes workers must have infected the inmates. This was 100 percent their fault. COVID was brought in by their people. The FCI Seagoville staff was not properly trained on how to handle this epidemic. Outbreaks like the one at Seagoville are happening across the country, more than 10,000 federal inmates infected or recovering, 110 are dead. According to Dr. Homer Venters, a correctional health expert who investigates COVID response in prisons, inmates are confined in crowded conditions, subject to exposure by staff that he believes are untrained or simply careless. And obviously, the prisoners, the inmates who cannot leave the facility are sitting ducks. That's absolutely right. and many of them are at high risk for serious illness or death if and when they contract COVID-19. And when prisons like Seagoville become infection hot spots, the staff who leave prison every day can be carrying more infection back into the communities where they live. Venters says at the very least, the most high-risk inmates need to be protected. Many lives could be saved and they still can be saved if we can find people who are high risk and get them out. Federal prisoners can apply for early release and the Bureau of Prisons can also identify vulnerable inmates and release them under home confinement. It's happened to President Trump's former campaign manager Paul Manafort, Trump's former personal attorney Michael Cohen. Even the Rapper Tekashi 6ix9ine was able to leave prison early during the pandemic. Critics say many, many more are being overlooked. George Reagan has a heart condition, according to his wife, who says he applied for compassionate release, was denied even though he's scheduled to get out of prison in four months. I 100 percent don't think that the federal government cares at all. Now we have three deaths in one month. One of those deaths, James Giannetta, who asked his brother, Russell, to help him. James Giannetta was 65, HIV positive, diabetic. They wrote the CDC, asked the prison, then the courts for compassionate release. It was just too late. James got infected, was hospitalized and was soon saying goodbye to his brother via FaceTime. He got into this business of where to scatter his ashes and all that kind of stuff, to which I said I don't want to think about that. Let's think about surviving. Listen to the doctors. He died July 16th. The Bureau of Prisons declined repeated requests for an interview. A spokesperson saying in a statement the agency has begun mass testing for COVID-19. Inmates are assessed for symptoms twice daily and cloth face coverings were issued to all staff and inmates. The low security men's prison is now a cautionary tale for how quickly the coronavirus can ravage correctional facilities -- Drew Griffin, CNN, Atlanta. Beirut, Lebanon's busy port, now in ruins. Just ahead, we'll take you to ground zero of Tuesday's horrifying explosion.\n",
      "\n",
      "Summary:\n",
      "Coronavirus Spreads In Texas Federal Prison\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Article ID: CNN-34632\n",
      "Text:\n",
      "The world will be finding out this morning just what city will host the 2008 Olympic Games. Members of the International Olympic Committee are choosing between five candidates. And if you're curious, in Las Vegas, odds makers have their money on Beijing. Well, CNN's Patrick Snell is covering the IOC meeting. He joins us now, live, from Moscow and that's where the announcement will be made in just a few hours. Hello there, what can you tell us? Linda, well welcome to the Russian capital. The very latest as the tension mounts here, the excitement we get all the more closer to who will win the right to stage the 2008 Summer Games. Let me bring you up to date what's happened so far today, Friday, in Moscow. We've had three presentations from the bid cities. The first of which was the Japanese bid from Osaka. Now, in this bid, the Japanese are claiming that they will provide the best sporting facilities and infrastructure in the world. And what's different -- what's innovative about this bid is that the facilities will be built across three uniquely designed manmade islands. So that's something that the Japanese are confident about. They're doubly more confident because, of course, they'll be staging next summer's football World Cup along with South Korea. So they're very, very confident that this is the bid that should catch the public imagination and hopefully will try and pull in, from their point of view, these 122 IOC votes that are up for grabs. Now what else do we have today? Well, we've had the bid from the French capital Paris. Yes, Oui Paris 2008. The French believe the Olympic Games should come to their city. Remember, they staged the 1998 football World Cup and won many plaudits for doing so, a wonderfully spectacular performance. They say their infrastructure is in place. They've got the Parisian Stadium, the Stade de France already built, infrastructure ready to go. And because they're promising games like Patrick Snell, thank you very much, from Moscow. TO ORDER A VIDEO OF THIS TRANSCRIPT, PLEASE CALL 800-CNN-NEWS OR USE OUR SECURE ONLINE ORDER FORM LOCATED AT www.fdch.com\n",
      "\n",
      "Summary:\n",
      "IOC Meeting in Russia to Choose Host City for 2008\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Wybierz dwa losowe przykłady\n",
    "random_samples = random.sample(extracted_samples, 2)\n",
    "\n",
    "# Wyświetl dwa losowe przykłady\n",
    "for i, sample in enumerate(random_samples):  \n",
    "    print(f\"Article ID: {sample['id']}\")\n",
    "    print(f\"Text:\\n{sample['text']}\\n\")\n",
    "    print(f\"Summary:\\n{sample['summary']}\\n{'-'*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liczenie tokenów w tekście\n",
    "\n",
    "Ten fragment kodu wykorzystuje tokenizer z biblioteki `transformers`, aby policzyć liczbę tokenów w podanym tekście. Jest to przydatne w przetwarzaniu języka naturalnego (NLP), szczególnie przy pracy z modelami opartymi na transformerach, takich jak BART.\n",
    "\n",
    "### Szczegółowy opis:\n",
    "\n",
    "1. **Funkcja `count_tokens`**:\n",
    "   - Liczy liczbę tokenów w podanym tekście przy użyciu tokenizera `facebook/bart-base`.\n",
    "   - Argumenty:\n",
    "     - `text` (str): Tekst wejściowy, który ma zostać tokenizowany.\n",
    "   - Zwraca liczbę tokenów w postaci liczby całkowitej (`int`).\n",
    "\n",
    "2. **Użycie tokenizera BART**:\n",
    "   - Tokenizer `BartTokenizer` jest inicjalizowany z pretrenowanego modelu `facebook/bart-base`.\n",
    "\n",
    "3. **Przykład użycia**:\n",
    "   - Dla tekstu `\"\"\" example text... \"\"\"` liczba tokenów jest obliczana i wyświetlana w konsoli.\n",
    "\n",
    "### Wynik:\n",
    "- Kod wyświetla liczbę tokenów w podanym tekście, co pomaga w analizie i przygotowaniu danych do przetwarzania przez modele NLP.\n",
    "\n",
    "### Uwagi:\n",
    "- Kod wykorzystuje bibliotekę `transformers` firmy Hugging Face, co oznacza, że należy ją zainstalować przed uruchomieniem kodu (`pip install transformers`).\n",
    "- Funkcja jest elastyczna i może być używana do analizy dowolnego tekstu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 452\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"\n",
    "    Counts the number of tokens in the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of tokens in the text.\n",
    "    \"\"\"\n",
    "    # Initialize the BART tokenizer\n",
    "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "    \n",
    "    # Tokenize the input text and return the token count\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "# Example usage\n",
    "example_text = \"\"\"The world will be finding out this morning just what city will host the 2008 Olympic Games. Members of the International Olympic Committee are choosing between five candidates. And if you're curious, in Las Vegas, odds makers have their money on Beijing. Well, CNN's Patrick Snell is covering the IOC meeting. He joins us now, live, from Moscow and that's where the announcement will be made in just a few hours. Hello there, what can you tell us? Linda, well welcome to the Russian capital. The very latest as the tension mounts here, the excitement we get all the more closer to who will win the right to stage the 2008 Summer Games. Let me bring you up to date what's happened so far today, Friday, in Moscow. We've had three presentations from the bid cities. The first of which was the Japanese bid from Osaka. Now, in this bid, the Japanese are claiming that they will provide the best sporting facilities and infrastructure in the world. And what's different -- what's innovative about this bid is that the facilities will be built across three uniquely designed manmade islands. So that's something that the Japanese are confident about. They're doubly more confident because, of course, they'll be staging next summer's football World Cup along with South Korea. So they're very, very confident that this is the bid that should catch the public imagination and hopefully will try and pull in, from their point of view, these 122 IOC votes that are up for grabs. Now what else do we have today? Well, we've had the bid from the French capital Paris. Yes, Oui Paris 2008. The French believe the Olympic Games should come to their city. Remember, they staged the 1998 football World Cup and won many plaudits for doing so, a wonderfully spectacular performance. They say their infrastructure is in place. They've got the Parisian Stadium, the Stade de France already built, infrastructure ready to go. And because they're promising games like Patrick Snell, thank you very much, from Moscow. TO ORDER A VIDEO OF THIS TRANSCRIPT, PLEASE CALL 800-CNN-NEWS OR USE OUR SECURE ONLINE ORDER FORM LOCATED AT www.fdch.com\n",
    "\"\"\"\n",
    "token_count = count_tokens(example_text)\n",
    "print(f\"Number of tokens: {token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrowanie i podział artykułów\n",
    "\n",
    "Ten fragment kodu wykonuje filtrowanie artykułów na podstawie długości tekstu i streszczenia (mierzonej liczbą tokenów), a następnie dzieli dane na zbiory treningowe, walidacyjne i testowe. Wyniki są zapisywane do plików JSON.\n",
    "\n",
    "### Szczegółowy opis:\n",
    "\n",
    "1. **Filtrowanie artykułów (`filter_articles_by_token_length`)**:\n",
    "   - Artykuły są filtrowane na podstawie dwóch kryteriów:\n",
    "     - Maksymalna liczba tokenów w tekście (`max_tokens`).\n",
    "     - Minimalna liczba tokenów w streszczeniu (`min_summary_tokens`).\n",
    "   - Wykorzystywany jest tokenizer BART-a (`facebook/bart-base`), aby przeliczyć liczbę tokenów.\n",
    "\n",
    "2. **Podział i zapis danych (`split_and_save_filtered_data`)**:\n",
    "   - Dane są losowo tasowane i dzielone na trzy zbiory:\n",
    "     - Treningowy (`train_pct` - 60%).\n",
    "     - Walidacyjny (`val_pct` - 30%).\n",
    "     - Testowy (`test_pct` - 10%).\n",
    "   - Każdy zbiór jest zapisywany do osobnego pliku JSON w określonym folderze.\n",
    "\n",
    "3. **Ścieżki plików**:\n",
    "   - Plik wejściowy (`input_file`) zawiera dane wejściowe w formacie JSON.\n",
    "   - Folder wyjściowy (`output_folder`) przechowuje wyniki podziału danych.\n",
    "\n",
    "### Wynik:\n",
    "- Po przetworzeniu danych wyświetlana jest liczba artykułów po filtrowaniu oraz liczba próbek w każdym zbiorze.\n",
    "- Dane są zapisane do plików `train.json`, `val.json`, i `test.json`.\n",
    "\n",
    "### Uwagi:\n",
    "- Tokenizer `BartTokenizer` wymaga biblioteki Hugging Face Transformers (`pip install transformers`).\n",
    "- Wartość `max_tokens` i `min_summary_tokens` można dostosować do specyficznych wymagań projektu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering articles based on token counts...\n",
      "Number of articles after filtering: 24692\n",
      "Data saved in folder: ./datasets/splits_filtered_with_summary\n",
      "Sample counts: Training = 14815, Validation = 7407, Testing = 2470\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Initialize BART tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "def filter_articles_by_token_length(input_data, max_tokens=1024, min_summary_tokens=10):\n",
    "    \"\"\"\n",
    "    Filters articles so that the text has a maximum number of tokens, \n",
    "    and the summary has a minimum number of tokens.\n",
    "\n",
    "    Args:\n",
    "        input_data (list): List of articles with \"id\", \"text\", and \"summary\" keys.\n",
    "        max_tokens (int): Maximum number of tokens for the text.\n",
    "        min_summary_tokens (int): Minimum number of tokens for the summary.\n",
    "\n",
    "    Returns:\n",
    "        list: List of articles meeting the conditions.\n",
    "    \"\"\"\n",
    "    filtered_data = []\n",
    "    for article in input_data:\n",
    "        text_tokenized = tokenizer(article[\"text\"], truncation=False)[\"input_ids\"]\n",
    "        summary_tokenized = tokenizer(article[\"summary\"], truncation=False)[\"input_ids\"]\n",
    "        \n",
    "        # Check conditions: text and summary lengths\n",
    "        if len(text_tokenized) <= max_tokens and len(summary_tokenized) >= min_summary_tokens:\n",
    "            filtered_data.append(article)\n",
    "    return filtered_data\n",
    "\n",
    "def split_and_save_filtered_data(input_file, output_folder, max_tokens=1024, min_summary_tokens=10, train_pct=0.6, val_pct=0.3, test_pct=0.1):\n",
    "    \"\"\"\n",
    "    Filters articles, splits them into training, validation, and test sets, \n",
    "    and saves the results to JSON files.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the JSON file with articles.\n",
    "        output_folder (str): Folder where results will be saved.\n",
    "        max_tokens (int): Maximum number of tokens for the text.\n",
    "        min_summary_tokens (int): Minimum number of tokens for the summary.\n",
    "        train_pct (float): Percentage of data for training.\n",
    "        val_pct (float): Percentage of data for validation.\n",
    "        test_pct (float): Percentage of data for testing.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load data from file\n",
    "    with open(input_file, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Filter articles\n",
    "    print(\"Filtering articles based on token counts...\")\n",
    "    filtered_data = filter_articles_by_token_length(\n",
    "        data, max_tokens=max_tokens, min_summary_tokens=min_summary_tokens\n",
    "    )\n",
    "    print(f\"Number of articles after filtering: {len(filtered_data)}\")\n",
    "\n",
    "    # Shuffle and split data\n",
    "    random.shuffle(filtered_data)\n",
    "    total_samples = len(filtered_data)\n",
    "    train_count = int(total_samples * train_pct)\n",
    "    val_count = int(total_samples * val_pct)\n",
    "    train_data = filtered_data[:train_count]\n",
    "    val_data = filtered_data[train_count:train_count + val_count]\n",
    "    test_data = filtered_data[train_count + val_count:]\n",
    "\n",
    "    # Create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Save splits to JSON files\n",
    "    with open(os.path.join(output_folder, \"train.json\"), \"w\") as train_file:\n",
    "        json.dump(train_data, train_file, indent=4, ensure_ascii=False)\n",
    "    with open(os.path.join(output_folder, \"val.json\"), \"w\") as val_file:\n",
    "        json.dump(val_data, val_file, indent=4, ensure_ascii=False)\n",
    "    with open(os.path.join(output_folder, \"test.json\"), \"w\") as test_file:\n",
    "        json.dump(test_data, test_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Data saved in folder: {output_folder}\")\n",
    "    print(f\"Sample counts: Training = {len(train_data)}, Validation = {len(val_data)}, Testing = {len(test_data)}\")\n",
    "\n",
    "# File paths\n",
    "input_file = \"./datasets/extracted_samples.json\"  # File with previously extracted data\n",
    "output_folder = \"./datasets/splits_filtered_with_summary\"  # Folder for results\n",
    "\n",
    "# Filter and save data\n",
    "split_and_save_filtered_data(\n",
    "    input_file, output_folder, max_tokens=1024, min_summary_tokens=40, train_pct=0.6, val_pct=0.3, test_pct=0.1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
