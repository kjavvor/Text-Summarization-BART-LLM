{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "This code segment is responsible for preparing the dataset. It loads JSON files containing the training, validation, and test split information, as well as a large file with dialogue data. The code is optimized for handling large JSON files, ensuring efficient memory usage.\n",
    "\n",
    "### Detailed description:\n",
    "\n",
    "1. **File path definitions**:\n",
    "   - `SPLIT_FILE_PATH`: Path to the JSON file that contains the train/validation/test data split.\n",
    "   - `NEWSDIALOG_FILE_PATH`: Path to the large JSON file containing dialogue data.\n",
    "\n",
    "2. **Loading split data**:\n",
    "   - The contents of the `train_val_test_split.json` file are loaded into the `split_data` variable using the `json` library.\n",
    "\n",
    "3. **Function to load a large JSON file**:\n",
    "   - `load_large_json(file_path)` is a generator function that reads a JSON file line by line, allowing large datasets to be processed without loading the entire file into memory.\n",
    "\n",
    "4. **Confirmation of successful data loading**:\n",
    "   - After the data is loaded, the script prints the message: \"Files have been successfully loaded.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files have been successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "# File paths\n",
    "SPLIT_FILE_PATH = \"/Users/kamiljaworski/Projects/NLP_pro/datasets/MediaSum/data/train_val_test_split.json\"\n",
    "NEWSDIALOG_FILE_PATH = \"/Users/kamiljaworski/Projects/NLP_pro/datasets/MediaSum/data/news_dialogue.json\"\n",
    "\n",
    "# Load split data\n",
    "with open(SPLIT_FILE_PATH, \"r\") as split_file:\n",
    "    split_data = json.load(split_file)\n",
    "\n",
    "# Function to load a large JSON file line by line\n",
    "def load_large_json(file_path):\n",
    "    \"\"\"\n",
    "    Generator function to load a large JSON file line by line.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the JSON file.\n",
    "        \n",
    "    Yields:\n",
    "        dict: Parsed JSON object for each line in the file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            yield json.loads(line)\n",
    "\n",
    "print(\"Files have been successfully loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Data from Splits\n",
    "\n",
    "This code segment is responsible for sampling a specified number of data points from the combined `train`, `val`, and `test` datasets. It is useful when you need a randomized subset of the data for experimentation or evaluation purposes.\n",
    "\n",
    "### Detailed description:\n",
    "\n",
    "1. **Function `sample_train_ids`**:\n",
    "   - Randomly selects a specified number of IDs from the combined `train`, `val`, and `test` splits.\n",
    "   - **Arguments**:\n",
    "     - `split_data`: A dictionary containing the dataset splits.\n",
    "     - `sample_size`: The number of samples to retrieve.\n",
    "   - The function prints the total number of available unique IDs and raises a `ValueError` if the requested sample size exceeds the number of available IDs.\n",
    "\n",
    "2. **Combining IDs from all splits**:\n",
    "   - The IDs from the `train`, `val`, and `test` sets are merged into a single list.\n",
    "\n",
    "3. **Random sampling**:\n",
    "   - The `random.sample` function is used to select the specified number of unique IDs without repetition.\n",
    "\n",
    "4. **Global variables**:\n",
    "   - `accumulated_ids`: Total number of IDs across all dataset splits.\n",
    "   - `SAMPLE_SIZE`: The desired sample size, here set to the total number of IDs (i.e., using the full dataset).\n",
    "\n",
    "5. **Output**:\n",
    "   - After sampling, the script prints the number of selected samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique IDs: 463596\n",
      "Selected 463596 samples.\n"
     ]
    }
   ],
   "source": [
    "def sample_train_ids(split_data, sample_size):\n",
    "    \"\"\"\n",
    "    Selects a specified number of samples from the `train`, `val`, and `test` datasets.\n",
    "\n",
    "    Args:\n",
    "        split_data (dict): Dictionary containing split data.\n",
    "        sample_size (int): Number of samples to select.\n",
    "\n",
    "    Returns:\n",
    "        list: List of selected IDs.\n",
    "    \"\"\"\n",
    "    # Combine IDs from train, validation, and test splits\n",
    "    train_ids = split_data[\"train\"] + split_data[\"val\"] + split_data[\"test\"]\n",
    "    \n",
    "    # Print the number of unique IDs\n",
    "    print(f\"Number of unique IDs: {len(train_ids)}\")\n",
    "    \n",
    "    # Raise an error if the sample size exceeds the total number of IDs\n",
    "    if sample_size > len(train_ids):\n",
    "        raise ValueError(\n",
    "            f\"Sample size ({sample_size}) exceeds the number of available data points ({len(train_ids)}).\"\n",
    "        )\n",
    "    \n",
    "    # Randomly sample the specified number of IDs\n",
    "    return random.sample(train_ids, sample_size)\n",
    "\n",
    "# Calculate the total number of IDs across all splits\n",
    "accumulated_ids = len(split_data[\"train\"] + split_data[\"val\"] + split_data[\"test\"])\n",
    "\n",
    "# Define the sample size as the total number of IDs\n",
    "SAMPLE_SIZE = accumulated_ids\n",
    "\n",
    "# Select a sample of IDs\n",
    "sampled_ids = sample_train_ids(split_data, SAMPLE_SIZE)\n",
    "\n",
    "# Print the number of selected samples\n",
    "print(f\"Selected {len(sampled_ids)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Data by ID\n",
    "\n",
    "This code segment extracts specific articles and their summaries based on a list of selected IDs. It processes the main JSON dataset and saves the extracted samples to a new file. This approach is useful when working with a smaller, targeted subset of the data.\n",
    "\n",
    "### Detailed description:\n",
    "\n",
    "1. **Function `extract_data_by_ids`**:\n",
    "   - Extracts article data (ID, full text, and summary) from the original dataset based on a given list of IDs.\n",
    "   - **Arguments**:\n",
    "     - `news_file_path`: Path to the source JSON file containing all article data.\n",
    "     - `ids_to_extract`: List of selected IDs to extract from the dataset.\n",
    "   - The function:\n",
    "     - Converts the list of IDs to a set for faster lookups.\n",
    "     - Loads the entire JSON file into memory (assuming it's manageable).\n",
    "     - Iterates over each article and checks if its ID is among the selected ones.\n",
    "     - Extracts and formats the article's `utt` (utterances) into a single string of text, along with the summary.\n",
    "     - Stops early once all desired IDs have been found.\n",
    "\n",
    "2. **Extracting data**:\n",
    "   - The function is called with `NEWSDIALOG_FILE_PATH` and `sampled_ids` as inputs.\n",
    "   - Extracted samples are stored in `extracted_samples`.\n",
    "\n",
    "3. **Saving to file**:\n",
    "   - The extracted samples are saved to a new JSON file at `./datasets/extracted_samples.json` with proper formatting (`indent=4`).\n",
    "\n",
    "4. **Output messages**:\n",
    "   - Prints the number of extracted articles.\n",
    "   - Confirms the location where the new file is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 463596 articles.\n",
      "Extracted data saved to file: ./datasets/extracted_samples.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def extract_data_by_ids(news_file_path, ids_to_extract):\n",
    "    \"\"\"\n",
    "    Extracts articles and summaries based on provided IDs.\n",
    "\n",
    "    Args:\n",
    "        news_file_path (str): Path to the NewsDialog.json file.\n",
    "        ids_to_extract (list): List of IDs to extract.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of articles containing ID, text, and summary.\n",
    "    \"\"\"\n",
    "    extracted_data = []\n",
    "    ids_set = set(ids_to_extract)  # Use a set for faster lookups\n",
    "\n",
    "    # Open and load the JSON file\n",
    "    with open(news_file_path, \"r\") as file:\n",
    "        news_data = json.load(file)  # Load the entire JSON file (list)\n",
    "\n",
    "        for article in news_data:\n",
    "            # Check if \"id\" exists and matches one of the selected IDs\n",
    "            if \"id\" in article and article[\"id\"] in ids_set:\n",
    "                extracted_data.append({\n",
    "                    \"id\": article[\"id\"],\n",
    "                    \"text\": \" \".join(article.get(\"utt\", [])),  # Join sentences\n",
    "                    \"summary\": article.get(\"summary\", \"\")  # Get the summary\n",
    "                })\n",
    "            # Stop when all IDs have been processed\n",
    "            if len(extracted_data) == len(ids_to_extract):\n",
    "                break  \n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "# Extract data\n",
    "extracted_samples = extract_data_by_ids(NEWSDIALOG_FILE_PATH, sampled_ids)\n",
    "print(f\"Extracted {len(extracted_samples)} articles.\")\n",
    "\n",
    "# Save extracted data to a JSON file\n",
    "output_file = \"./datasets/extracted_samples.json\"\n",
    "with open(output_file, \"w\") as file:\n",
    "    json.dump(extracted_samples, file, indent=4)\n",
    "print(f\"Extracted data saved to file: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying Random Sampled Examples\n",
    "\n",
    "This code segment randomly selects and displays a few examples from the extracted dataset. It is useful for quickly verifying the quality and structure of the extracted data by visually inspecting a small subset.\n",
    "\n",
    "### Detailed description:\n",
    "\n",
    "1. **Random selection**:\n",
    "   - Two examples are randomly selected from the `extracted_samples` list using `random.sample`.\n",
    "\n",
    "2. **Output formatting**:\n",
    "   - Each selected article is displayed with the following fields:\n",
    "     - `Article ID`: The unique identifier of the article.\n",
    "     - `Text`: The full concatenated article text.\n",
    "     - `Summary`: The corresponding summary.\n",
    "   - A horizontal separator line (`'-'*100`) is printed after each sample for clarity.\n",
    "\n",
    "3. **Loop through samples**:\n",
    "   - The `enumerate` function is used to iterate over the randomly selected samples and print their contents in a structured way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article ID: CNN-407734\n",
      "Text:\n",
      "As the total number of coronavirus cases in the U.S. nears 5 million, we're now learning just how quickly the virus is spreading inside the U.S. federal prison system. CNN's Drew Griffin has our report. The Seagoville Federal Correctional Institution outside Dallas is a petri dish of coronavirus infection. I lost my smell. I lived in the restroom for like 12 days. In the 15-minute phone call allowed from the inside, inmate George Reagan explained how coronavirus swept through the facility in just a month. Everybody has it now. Seagoville has significantly more COVID-19 cases than any other federal prison in the U.S. More than 1,300 of the roughly 1,750 inmates there tested positive. That's 75 percent. George Reagan's wife, Tabitha, that says no visitors have been allowed at the prison for months, so she believes workers must have infected the inmates. This was 100 percent their fault. COVID was brought in by their people. The FCI Seagoville staff was not properly trained on how to handle this epidemic. Outbreaks like the one at Seagoville are happening across the country, more than 10,000 federal inmates infected or recovering, 110 are dead. According to Dr. Homer Venters, a correctional health expert who investigates COVID response in prisons, inmates are confined in crowded conditions, subject to exposure by staff that he believes are untrained or simply careless. And obviously, the prisoners, the inmates who cannot leave the facility are sitting ducks. That's absolutely right. and many of them are at high risk for serious illness or death if and when they contract COVID-19. And when prisons like Seagoville become infection hot spots, the staff who leave prison every day can be carrying more infection back into the communities where they live. Venters says at the very least, the most high-risk inmates need to be protected. Many lives could be saved and they still can be saved if we can find people who are high risk and get them out. Federal prisoners can apply for early release and the Bureau of Prisons can also identify vulnerable inmates and release them under home confinement. It's happened to President Trump's former campaign manager Paul Manafort, Trump's former personal attorney Michael Cohen. Even the Rapper Tekashi 6ix9ine was able to leave prison early during the pandemic. Critics say many, many more are being overlooked. George Reagan has a heart condition, according to his wife, who says he applied for compassionate release, was denied even though he's scheduled to get out of prison in four months. I 100 percent don't think that the federal government cares at all. Now we have three deaths in one month. One of those deaths, James Giannetta, who asked his brother, Russell, to help him. James Giannetta was 65, HIV positive, diabetic. They wrote the CDC, asked the prison, then the courts for compassionate release. It was just too late. James got infected, was hospitalized and was soon saying goodbye to his brother via FaceTime. He got into this business of where to scatter his ashes and all that kind of stuff, to which I said I don't want to think about that. Let's think about surviving. Listen to the doctors. He died July 16th. The Bureau of Prisons declined repeated requests for an interview. A spokesperson saying in a statement the agency has begun mass testing for COVID-19. Inmates are assessed for symptoms twice daily and cloth face coverings were issued to all staff and inmates. The low security men's prison is now a cautionary tale for how quickly the coronavirus can ravage correctional facilities -- Drew Griffin, CNN, Atlanta. Beirut, Lebanon's busy port, now in ruins. Just ahead, we'll take you to ground zero of Tuesday's horrifying explosion.\n",
      "\n",
      "Summary:\n",
      "Coronavirus Spreads In Texas Federal Prison\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Article ID: CNN-34632\n",
      "Text:\n",
      "The world will be finding out this morning just what city will host the 2008 Olympic Games. Members of the International Olympic Committee are choosing between five candidates. And if you're curious, in Las Vegas, odds makers have their money on Beijing. Well, CNN's Patrick Snell is covering the IOC meeting. He joins us now, live, from Moscow and that's where the announcement will be made in just a few hours. Hello there, what can you tell us? Linda, well welcome to the Russian capital. The very latest as the tension mounts here, the excitement we get all the more closer to who will win the right to stage the 2008 Summer Games. Let me bring you up to date what's happened so far today, Friday, in Moscow. We've had three presentations from the bid cities. The first of which was the Japanese bid from Osaka. Now, in this bid, the Japanese are claiming that they will provide the best sporting facilities and infrastructure in the world. And what's different -- what's innovative about this bid is that the facilities will be built across three uniquely designed manmade islands. So that's something that the Japanese are confident about. They're doubly more confident because, of course, they'll be staging next summer's football World Cup along with South Korea. So they're very, very confident that this is the bid that should catch the public imagination and hopefully will try and pull in, from their point of view, these 122 IOC votes that are up for grabs. Now what else do we have today? Well, we've had the bid from the French capital Paris. Yes, Oui Paris 2008. The French believe the Olympic Games should come to their city. Remember, they staged the 1998 football World Cup and won many plaudits for doing so, a wonderfully spectacular performance. They say their infrastructure is in place. They've got the Parisian Stadium, the Stade de France already built, infrastructure ready to go. And because they're promising games like Patrick Snell, thank you very much, from Moscow. TO ORDER A VIDEO OF THIS TRANSCRIPT, PLEASE CALL 800-CNN-NEWS OR USE OUR SECURE ONLINE ORDER FORM LOCATED AT www.fdch.com\n",
      "\n",
      "Summary:\n",
      "IOC Meeting in Russia to Choose Host City for 2008\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Wybierz dwa losowe przykłady\n",
    "random_samples = random.sample(extracted_samples, 2)\n",
    "\n",
    "# Wyświetl dwa losowe przykłady\n",
    "for i, sample in enumerate(random_samples):  \n",
    "    print(f\"Article ID: {sample['id']}\")\n",
    "    print(f\"Text:\\n{sample['text']}\\n\")\n",
    "    print(f\"Summary:\\n{sample['summary']}\\n{'-'*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Tokens with BART Tokenizer\n",
    "\n",
    "This code segment defines a utility function to count the number of tokens in a given text using the BART tokenizer from Hugging Face's Transformers library. Token counting is essential in NLP tasks, especially for models that have a maximum input length constraint.\n",
    "\n",
    "### Detailed description:\n",
    "\n",
    "1. **Function `count_tokens`**:\n",
    "   - Initializes the BART tokenizer (`facebook/bart-base`).\n",
    "   - Encodes the input text and returns the number of tokens.\n",
    "   - **Arguments**:\n",
    "     - `text`: A string representing the input to be tokenized.\n",
    "   - **Returns**:\n",
    "     - An integer representing the number of tokens in the input text.\n",
    "\n",
    "2. **Example usage**:\n",
    "   - A sample news transcript is provided as `example_text`.\n",
    "   - The function `count_tokens` is called on this text to calculate its token length.\n",
    "   - The result is printed as: `\"Number of tokens: X\"`.\n",
    "\n",
    "3. **Use case**:\n",
    "   - Useful for determining whether a text fits within a model’s maximum token limit, or for understanding input size before fine-tuning or inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 452\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"\n",
    "    Counts the number of tokens in the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of tokens in the text.\n",
    "    \"\"\"\n",
    "    # Initialize the BART tokenizer\n",
    "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "    \n",
    "    # Tokenize the input text and return the token count\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "# Example usage\n",
    "example_text = \"\"\"The world will be finding out this morning just what city will host the 2008 Olympic Games. Members of the International Olympic Committee are choosing between five candidates. And if you're curious, in Las Vegas, odds makers have their money on Beijing. Well, CNN's Patrick Snell is covering the IOC meeting. He joins us now, live, from Moscow and that's where the announcement will be made in just a few hours. Hello there, what can you tell us? Linda, well welcome to the Russian capital. The very latest as the tension mounts here, the excitement we get all the more closer to who will win the right to stage the 2008 Summer Games. Let me bring you up to date what's happened so far today, Friday, in Moscow. We've had three presentations from the bid cities. The first of which was the Japanese bid from Osaka. Now, in this bid, the Japanese are claiming that they will provide the best sporting facilities and infrastructure in the world. And what's different -- what's innovative about this bid is that the facilities will be built across three uniquely designed manmade islands. So that's something that the Japanese are confident about. They're doubly more confident because, of course, they'll be staging next summer's football World Cup along with South Korea. So they're very, very confident that this is the bid that should catch the public imagination and hopefully will try and pull in, from their point of view, these 122 IOC votes that are up for grabs. Now what else do we have today? Well, we've had the bid from the French capital Paris. Yes, Oui Paris 2008. The French believe the Olympic Games should come to their city. Remember, they staged the 1998 football World Cup and won many plaudits for doing so, a wonderfully spectacular performance. They say their infrastructure is in place. They've got the Parisian Stadium, the Stade de France already built, infrastructure ready to go. And because they're promising games like Patrick Snell, thank you very much, from Moscow. TO ORDER A VIDEO OF THIS TRANSCRIPT, PLEASE CALL 800-CNN-NEWS OR USE OUR SECURE ONLINE ORDER FORM LOCATED AT www.fdch.com\n",
    "\"\"\"\n",
    "token_count = count_tokens(example_text)\n",
    "print(f\"Number of tokens: {token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering and Splitting Articles by Token Length\n",
    "\n",
    "This code segment filters the dataset to remove overly long texts and very short summaries based on token counts using the BART tokenizer. It then splits the cleaned data into training, validation, and test sets, and saves them to separate JSON files.\n",
    "\n",
    "### Detailed description:\n",
    "\n",
    "1. **Tokenizer initialization**:\n",
    "   - The BART tokenizer (`facebook/bart-base`) from Hugging Face is loaded to tokenize both article texts and summaries.\n",
    "\n",
    "2. **Function `filter_articles_by_token_length`**:\n",
    "   - Filters articles based on two criteria:\n",
    "     - The number of tokens in the article text must be less than or equal to `max_tokens`.\n",
    "     - The number of tokens in the summary must be greater than or equal to `min_summary_tokens`.\n",
    "   - Uses `tokenizer(..., truncation=False)` to count tokens without truncating the input.\n",
    "   - Returns a list of articles that meet both conditions.\n",
    "\n",
    "3. **Function `split_and_save_filtered_data`**:\n",
    "   - Loads the article dataset from the input file.\n",
    "   - Applies the filtering function described above.\n",
    "   - Randomly shuffles the filtered data.\n",
    "   - Splits the data into training, validation, and test subsets using the provided percentages:\n",
    "     - `train_pct`: 60% of data\n",
    "     - `val_pct`: 30% of data\n",
    "     - `test_pct`: 10% of data\n",
    "   - Creates the output directory if it doesn't exist.\n",
    "   - Saves each data split (`train.json`, `val.json`, `test.json`) in the output folder with proper JSON formatting.\n",
    "\n",
    "4. **Execution**:\n",
    "   - The script runs the filtering and splitting logic with the following parameters:\n",
    "     - `max_tokens = 1024`: maximum allowed token length for article text.\n",
    "     - `min_summary_tokens = 40`: minimum token length required for summaries.\n",
    "   - Filtered and split data is saved under `./datasets/splits_filtered_with_summary`.\n",
    "\n",
    "5. **Output**:\n",
    "   - Prints the number of articles that passed the filtering step.\n",
    "   - Prints how many samples were saved to each of the training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering articles based on token counts...\n",
      "Number of articles after filtering: 24692\n",
      "Data saved in folder: ./datasets/splits_filtered_with_summary\n",
      "Sample counts: Training = 14815, Validation = 7407, Testing = 2470\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Initialize BART tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "def filter_articles_by_token_length(input_data, max_tokens=1024, min_summary_tokens=10):\n",
    "    \"\"\"\n",
    "    Filters articles so that the text has a maximum number of tokens, \n",
    "    and the summary has a minimum number of tokens.\n",
    "\n",
    "    Args:\n",
    "        input_data (list): List of articles with \"id\", \"text\", and \"summary\" keys.\n",
    "        max_tokens (int): Maximum number of tokens for the text.\n",
    "        min_summary_tokens (int): Minimum number of tokens for the summary.\n",
    "\n",
    "    Returns:\n",
    "        list: List of articles meeting the conditions.\n",
    "    \"\"\"\n",
    "    filtered_data = []\n",
    "    for article in input_data:\n",
    "        text_tokenized = tokenizer(article[\"text\"], truncation=False)[\"input_ids\"]\n",
    "        summary_tokenized = tokenizer(article[\"summary\"], truncation=False)[\"input_ids\"]\n",
    "        \n",
    "        # Check conditions: text and summary lengths\n",
    "        if len(text_tokenized) <= max_tokens and len(summary_tokenized) >= min_summary_tokens:\n",
    "            filtered_data.append(article)\n",
    "    return filtered_data\n",
    "\n",
    "def split_and_save_filtered_data(input_file, output_folder, max_tokens=1024, min_summary_tokens=10, train_pct=0.6, val_pct=0.3, test_pct=0.1):\n",
    "    \"\"\"\n",
    "    Filters articles, splits them into training, validation, and test sets, \n",
    "    and saves the results to JSON files.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the JSON file with articles.\n",
    "        output_folder (str): Folder where results will be saved.\n",
    "        max_tokens (int): Maximum number of tokens for the text.\n",
    "        min_summary_tokens (int): Minimum number of tokens for the summary.\n",
    "        train_pct (float): Percentage of data for training.\n",
    "        val_pct (float): Percentage of data for validation.\n",
    "        test_pct (float): Percentage of data for testing.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load data from file\n",
    "    with open(input_file, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Filter articles\n",
    "    print(\"Filtering articles based on token counts...\")\n",
    "    filtered_data = filter_articles_by_token_length(\n",
    "        data, max_tokens=max_tokens, min_summary_tokens=min_summary_tokens\n",
    "    )\n",
    "    print(f\"Number of articles after filtering: {len(filtered_data)}\")\n",
    "\n",
    "    # Shuffle and split data\n",
    "    random.shuffle(filtered_data)\n",
    "    total_samples = len(filtered_data)\n",
    "    train_count = int(total_samples * train_pct)\n",
    "    val_count = int(total_samples * val_pct)\n",
    "    train_data = filtered_data[:train_count]\n",
    "    val_data = filtered_data[train_count:train_count + val_count]\n",
    "    test_data = filtered_data[train_count + val_count:]\n",
    "\n",
    "    # Create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Save splits to JSON files\n",
    "    with open(os.path.join(output_folder, \"train.json\"), \"w\") as train_file:\n",
    "        json.dump(train_data, train_file, indent=4, ensure_ascii=False)\n",
    "    with open(os.path.join(output_folder, \"val.json\"), \"w\") as val_file:\n",
    "        json.dump(val_data, val_file, indent=4, ensure_ascii=False)\n",
    "    with open(os.path.join(output_folder, \"test.json\"), \"w\") as test_file:\n",
    "        json.dump(test_data, test_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Data saved in folder: {output_folder}\")\n",
    "    print(f\"Sample counts: Training = {len(train_data)}, Validation = {len(val_data)}, Testing = {len(test_data)}\")\n",
    "\n",
    "# File paths\n",
    "input_file = \"./datasets/extracted_samples.json\"  # File with previously extracted data\n",
    "output_folder = \"./datasets/splits_filtered_with_summary\"  # Folder for results\n",
    "\n",
    "# Filter and save data\n",
    "split_and_save_filtered_data(\n",
    "    input_file, output_folder, max_tokens=1024, min_summary_tokens=40, train_pct=0.6, val_pct=0.3, test_pct=0.1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
